{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = SparkConf().setAppName('DataSet').setMaster('local[2]')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame\n",
    "데이타프레임은 관계형 데이터베이스의 테이블에서 컬럼으로 구서된 병경 불가능한 분산 데이터 컬렉션이다.\n",
    "RDD의 약점 : 스파크 api를 파이썬에서 실행하는 것은 자바 jvm과 PY4J사이의 커뮤니케이션 오버헤드 발생으로 성능 저하가 생김"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringJSONRDD = sc.parallelize((\n",
    "    \"\"\"\n",
    "    {'id': '123',\n",
    "    'name': 'Katie',\n",
    "    'age': 19,\n",
    "    'eyeColor': 'brown'}\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    {'id': '234',\n",
    "    'name': 'Michael',\n",
    "    'age': 22,\n",
    "    'eyeColor': 'green'}\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    {'id': '345',\n",
    "    'name': 'Simone',\n",
    "    'age': 23,\n",
    "    'eyeColor': 'blue'}\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmersJSON = spark.read.json(stringJSONRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stringJSONRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(swimmersJSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(swimmersJSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmersJSON.createOrReplaceTempView('swimmersJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 19|   brown|123|  Katie|\n",
      "| 22|   green|234|Michael|\n",
      "| 23|    blue|345| Simone|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmersJSON.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n",
       " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
       " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('select * from swimmersJSON').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id='123', name='Katie')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('select * from swimmersJSON where id=\"123\"').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmersJSON.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringCSVRDD = sc.parallelize([\n",
    "    (123, 'Katie', 19, 'brown'),\n",
    "    (234, 'Michael', 22, 'green'),\n",
    "    (345, 'Simone', 23, 'blue')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('age', LongType(), True),\n",
    "    StructField('eyeColor', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmers = spark.createDataFrame(stringCSVRDD, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataBricks notebook에서 지원하는 매직 키워드\n",
    "%sql\n",
    "select * from swimmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmers.createOrReplaceTempView('swimmers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------+\n",
      "| id|   name|age|eyeColor|\n",
      "+---+-------+---+--------+\n",
      "|123|  Katie| 19|   brown|\n",
      "|234|Michael| 22|   green|\n",
      "|345| Simone| 23|    blue|\n",
      "+---+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from swimmers').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select count(*) from swimmers').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|123| 19|\n",
      "|234| 22|\n",
      "|345| 23|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select id, age from swimmers').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select id, age from swimmers where age = 22').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|eyeColor|\n",
      "+------+--------+\n",
      "| Katie|   brown|\n",
      "|Simone|    blue|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select name, eyeColor from swimmers where eyeColor like \"b%\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying with the DataFram API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string, age: bigint, eyeColor: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(swimmers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swimmers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|123| 19|\n",
      "|234| 22|\n",
      "|345| 23|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.select('id', 'age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.select('id', 'age').filter('age = 22').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "filightPerfFilePath = './departuredelays.csv'\n",
    "aiportsFilePath = './airport-codes-na.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark.read.csv()\n",
    "- 첫번째 : 데이터 파일 경로\n",
    "- 두번째 : 첫번째 줄이 헤더인지 여부\n",
    "- 세번째 : 스파크가리풀렉션으로 데이터 타입 추측 여부\n",
    "- 네번째 : 구분자 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = spark.read.csv(aiportsFilePath, header='true', inferSchema='true', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.createOrReplaceTempView('airports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightPerf = spark.read.csv(filightPerfFilePath, header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightPerf.createOrReplaceTempView('FlightPerformance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightPerf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[City\tState\tCountry\tIATA: string]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('com.databricks.spark.csv').option('header', 'true').load(aiportsFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Bellingham|   WA|    USA| BLI|\n",
      "| Moses Lake|   WA|    USA| MWH|\n",
      "|      Pasco|   WA|    USA| PSC|\n",
      "|    Pullman|   WA|    USA| PUW|\n",
      "|    Seattle|   WA|    USA| SEA|\n",
      "|    Spokane|   WA|    USA| GEG|\n",
      "|Walla Walla|   WA|    USA| ALW|\n",
      "|  Wenatchee|   WA|    USA| EAT|\n",
      "|     Yakima|   WA|    USA| YKM|\n",
      "+-----------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from airports where State = \"WA\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Bellingham|   WA|    USA| BLI|\n",
      "| Moses Lake|   WA|    USA| MWH|\n",
      "|      Pasco|   WA|    USA| PSC|\n",
      "|    Pullman|   WA|    USA| PUW|\n",
      "|    Seattle|   WA|    USA| SEA|\n",
      "|    Spokane|   WA|    USA| GEG|\n",
      "|Walla Walla|   WA|    USA| ALW|\n",
      "|  Wenatchee|   WA|    USA| EAT|\n",
      "|     Yakima|   WA|    USA| YKM|\n",
      "+-----------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.select('*').filter('State = \"WA\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|   City|origin|  Delays|\n",
      "+-------+------+--------+\n",
      "|Seattle|   SEA|159086.0|\n",
      "|Spokane|   GEG| 12404.0|\n",
      "|  Pasco|   PSC|   949.0|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 워싱턴에서 출발하는 비행기 중에서 지연이 됀 경로 검색\n",
    "spark.sql('select a.City, f.origin, sum(f.delay) as Delays \\\n",
    "            from FlightPerformance f \\\n",
    "            join airports a on a.IATA = f.origin \\\n",
    "            where a.State = \"WA\" \\\n",
    "            group by a.City, f.origin \\\n",
    "            order by sum(f.delay) desc').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+\n",
      "|           City|   Delays|\n",
      "+---------------+---------+\n",
      "|        Chicago|1193929.0|\n",
      "|        Atlanta|1151087.0|\n",
      "|         Denver| 899406.0|\n",
      "|         Dallas| 679249.0|\n",
      "|        Houston| 587978.0|\n",
      "|    Los Angeles| 565490.0|\n",
      "|  San Francisco| 501670.0|\n",
      "|         Newark| 452791.0|\n",
      "|        Orlando| 445070.0|\n",
      "|      Las Vegas| 441797.0|\n",
      "|        Chicago| 394254.0|\n",
      "|       New York| 387929.0|\n",
      "|        Phoenix| 364123.0|\n",
      "|      Baltimore| 362845.0|\n",
      "|        Detroit| 305426.0|\n",
      "|       New York| 303888.0|\n",
      "|Fort Lauderdale| 297358.0|\n",
      "|  Washington DC| 260151.0|\n",
      "|    Minneapolis| 250779.0|\n",
      "|      Charlotte| 246558.0|\n",
      "+---------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 미국의 주별 지연 정보 검색\n",
    "spark.sql('select a.City, sum(f.delay) as Delays \\\n",
    "            from FlightPerformance f \\\n",
    "            join airports a on a.IATA = f.origin \\\n",
    "            where a.Country = \"USA\" \\\n",
    "            group by a.City, f.origin \\\n",
    "            order by sum(f.delay) desc').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
