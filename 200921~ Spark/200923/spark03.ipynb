{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and understand data for modeling\n",
    "모든 데이터는 보통 지저분하고 데이터가 의도한 것에 대한 충분한 신뢰성을 가지고 있지 않다.  \n",
    "\n",
    "데이터는 중복 데이터나 관찰되지않는 값(결측치), 아웃라이어(이상치), 존재하지 않는 주소, 잘못된 전화번호 또는 지역코드, 올바르지 않은 직역좌표, 잘못된 테이터나 레이블, 대소문자 구분, 공백 관련 문제를 가지고있다.  \n",
    "\n",
    "데이터 과학자, 데이터 엔지니어는 통계모델 또는 머신러닝 모델을 빌드하기 위해 이러한 데이터를 깨끗하게 만들어야 한다.  \n",
    "\n",
    "데이터는 앞에서 말한 문제점들이 없을 경우 기술적으로 깨끗하다고 말 할 수 있다. 그러나 모델링 목적으로 데이터셋을 깨끗하게 하기 위해서는 피처의 분포를 확인해야 하고 사전에 정의된 조건들을 만족하는지 검증 해야 한다.\n",
    "\n",
    "**데이터과학자는 80 - 90%의 시간을 데이터를 다루거나 피처에 익숙해지는데 쓰게 된다**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = SparkConf().setAppName('appName1').setMaster('local[2]')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, 144.5, 5.9, 33, 'M'),\n",
    "    (2, 167.2, 5.4, 45, 'M'),\n",
    "    (3, 124.1, 5.2, 23, 'F'),\n",
    "    (4, 144.5, 5.9, 33, 'M'),\n",
    "    (5, 133.2, 5.7, 54, 'F'),\n",
    "    (3, 124.1, 5.2, 23, 'F'),\n",
    "    (5, 129.2, 5.3, 42, 'M')\n",
    "], ['id', 'weight', 'height', 'age', 'gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of rows: 6\n"
     ]
    }
   ],
   "source": [
    "print('count of rows: {0}'.format(df.distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  4| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.dropDuplicates()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of rows: 6\n"
     ]
    }
   ],
   "source": [
    "print('count of rows: {0}'.format(df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of distinct ids: 5\n"
     ]
    }
   ],
   "source": [
    "print('count of distinct ids: {0}'\\\n",
    "      .format(df.select([c for c in df.columns if c != 'id']).distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'weight', 'height', 'age', 'gender']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weight', 'height', 'age', 'gender']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c for c in df.columns if c != 'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.dropDuplicates(subset=[c for c in df.columns if c != 'id'])\n",
    "# df = df.dropDuplicates(subset=['weight', 'height', 'age', 'gender'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|count|distinct|\n",
      "+-----+--------+\n",
      "|    5|       4|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(\n",
    "        fn.count('id').alias('count'),\n",
    "        fn.countDistinct('id').alias('distinct')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "monotonically_increasing_id() : 각 행에 고유한 값을 부여 한다.  \n",
    "각각의 파티션에 800억개의 데이터가 있고 파티션의 수가 약 10억개 미만인 데이터들에 대해서는 안정적으로 공유한 ID값을 부여 해 준다. (스파크 이전 버전에서는 같은 데이터프레임에서 여러번 작업이 이뤄졌을때마다 ID값이 바뀌었다, 스파크 2.0에서는 고정된 ID값을 부여 해 준다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+-------------+\n",
      "| id|weight|height|age|gender|       new_id|\n",
      "+---+------+------+---+------+-------------+\n",
      "|  5| 133.2|   5.7| 54|     F|  25769803776|\n",
      "|  1| 144.5|   5.9| 33|     M| 171798691840|\n",
      "|  2| 167.2|   5.4| 45|     M| 592705486848|\n",
      "|  3| 124.1|   5.2| 23|     F|1236950581248|\n",
      "|  5| 129.2|   5.3| 42|     M|1365799600128|\n",
      "+---+------+------+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new_id', fn.monotonically_increasing_id()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing observations\n",
    "**미관찰 값들을 다룰 수 있는 가장 간단한 방법은 미관찰 값을 가지고있는 모든 데이터를 제거 하는 것이다.**  \n",
    "단 너무 많은 데이터를 제거하지 않도록 조심해야함.  \n",
    "데이터셋 전체에서 미관찰 값의 분포에 따라 데이터셋 전체의 사용 가능성에 큰 영향을 미치리 수 있다.  \n",
    "데이터를 제거한 후 아주 작은 데이터만 남거나, 데이터가 절반 이상으로 줄어들었다면 어떤 피처가 빈칸을 가장 많이 가지고있는지 확인하고 그 피처를 제거하는 것이 더 좋다.  \n",
    "\n",
    "미관찰 값을 다루는 또 다른 방법은 None으로 채우는 것이다. 이럴경우 데이터필드 타입에 딸라 몇몇 다양한 값으로 대체해 채워 넣을 수 있다.\n",
    "- 데이터가 참/거짓으로 구분되면 Missing라는 세번째 카테고리를 넣는다\n",
    "- 데이터가 이미 카테고리를 가지고 있다면 Missing카테고리를 추가한다\n",
    "- 순서 혹은 숫자 데이터를 가지고있을경우에는 평균, 중간값 또는 미리 정의된 다른 값으로 바꿀 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_miss = spark.createDataFrame([\n",
    "    (1, 144.5, 5.9, 33, 'M', 100000),\n",
    "    (2, 167.2, 5.4, 45, 'M', None),\n",
    "    (3, None, 5.2, None, None, None),\n",
    "    (4, 144.5, 5.9, 33, 'M', None),\n",
    "    (5, 133.2, 5.7, 54, 'F', None),\n",
    "    (6, 124.1, 5.2, None, 'F', None),\n",
    "    (7, 129.2, 5.3, 42, 'M', 76000)\n",
    "], ['id', 'weight', 'height', 'age', 'gender', 'income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0), (2, 1), (3, 4), (4, 1), (5, 1), (6, 2), (7, 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id별 결측치 갯수 확인\n",
    "df_miss.rdd.map(lambda row: (row['id'], sum([c == None for c in row]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+------+\n",
      "| id|weight|height| age|gender|income|\n",
      "+---+------+------+----+------+------+\n",
      "|  3|  null|   5.2|null|  null|  null|\n",
      "+---+------+------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss.where('id ==  3').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-------------+------------------+------------------+------------------+\n",
      "|idmissing|     weightmissing|heightmissing|        agemissing|     gendermissing|     incomemissing|\n",
      "+---------+------------------+-------------+------------------+------------------+------------------+\n",
      "|      0.0|0.1428571428571429|          0.0|0.2857142857142857|0.1428571428571429|0.7142857142857143|\n",
      "+---------+------------------+-------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 컬럼별 결측치 확인\n",
    "df_miss.agg(*[\n",
    "    (1 - (fn.count(c) / fn.count('*'))).alias(c + 'missing')\n",
    "    for c in df_miss.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_miss_no_income = df_miss.select([c for c in df_miss.columns if c != 'income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'weight', 'height', 'age', 'gender']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c for c in df_miss.columns if c != 'income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+\n",
      "| id|weight|height| age|gender|\n",
      "+---+------+------+----+------+\n",
      "|  1| 144.5|   5.9|  33|     M|\n",
      "|  2| 167.2|   5.4|  45|     M|\n",
      "|  3|  null|   5.2|null|  null|\n",
      "|  4| 144.5|   5.9|  33|     M|\n",
      "|  5| 133.2|   5.7|  54|     F|\n",
      "|  6| 124.1|   5.2|null|     F|\n",
      "|  7| 129.2|   5.3|  42|     M|\n",
      "+---+------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss_no_income.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'weight', 'height', 'age', 'gender', 'income']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_miss.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+\n",
      "| id|weight|height| age|gender|\n",
      "+---+------+------+----+------+\n",
      "|  1| 144.5|   5.9|  33|     M|\n",
      "|  2| 167.2|   5.4|  45|     M|\n",
      "|  3|  null|   5.2|null|  null|\n",
      "|  4| 144.5|   5.9|  33|     M|\n",
      "|  5| 133.2|   5.7|  54|     F|\n",
      "|  6| 124.1|   5.2|null|     F|\n",
      "|  7| 129.2|   5.3|  42|     M|\n",
      "+---+------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss.select(['id', 'weight', 'height', 'age', 'gender']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+\n",
      "| id|weight|height| age|gender|\n",
      "+---+------+------+----+------+\n",
      "|  1| 144.5|   5.9|  33|     M|\n",
      "|  2| 167.2|   5.4|  45|     M|\n",
      "|  4| 144.5|   5.9|  33|     M|\n",
      "|  5| 133.2|   5.7|  54|     F|\n",
      "|  6| 124.1|   5.2|null|     F|\n",
      "|  7| 129.2|   5.3|  42|     M|\n",
      "+---+------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss_no_income.dropna(thresh=3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미관찰 값을 추정해 채우려면 fillna()함수를 사용 할 수 있다.  \n",
    "이 함수는 단일 integer, float, long, string타입을 지원한다.  \n",
    "전체 데이터셋에서 미관찰값은 지정한 값으로 채워질 것이다.  \n",
    "평균, 중앙값 또는 다른 계산된 값으로 채우려면 우선 그 값을 계산하고 그 값을 가지는 딕셔너리를 만든 후 fillna()함수에 전달 하면된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = df_miss_no_income.agg(\n",
    "    *[fn.mean(c).alias(c) for c in df_miss_no_income.columns if c != 'gender']\n",
    ").toPandas().to_dict('records')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toPandas()함수는 collect()함수와같은 방식으로 동작한다, 데이터양에따라 문제가 발생할수있다. toPandas()함수는 모든 정보를 워커노트로부터 수집 후 드라이버 노드로 가지고 온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 4.0,\n",
       " 'weight': 140.45000000000002,\n",
       " 'height': 5.514285714285714,\n",
       " 'age': 41.4,\n",
       " 'gender': 'missing'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means['gender'] = 'missing'\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------+---+-------+\n",
      "| id|            weight|height|age| gender|\n",
      "+---+------------------+------+---+-------+\n",
      "|  1|             144.5|   5.9| 33|      M|\n",
      "|  2|             167.2|   5.4| 45|      M|\n",
      "|  3|140.45000000000002|   5.2| 41|missing|\n",
      "|  4|             144.5|   5.9| 33|      M|\n",
      "|  5|             133.2|   5.7| 54|      F|\n",
      "|  6|             124.1|   5.2| 41|      F|\n",
      "|  7|             129.2|   5.3| 42|      M|\n",
      "+---+------------------+------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss_no_income.fillna(means).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers = spark.createDataFrame([\n",
    "    (1, 144.5, 5.9, 33),\n",
    "    (2, 167.2, 5.4, 45),\n",
    "    (3, 342.3, 5.2, 99),\n",
    "    (4, 144.5, 5.9, 33),\n",
    "    (5, 133.2, 5.7, 54),\n",
    "    (6, 124.1, 5.2, 23),\n",
    "    (7, 129.2, 5.3, 42)\n",
    "], ['id', 'weight', 'height', 'age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_outliers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아웃라이어는 대부분의 데이터와는 매우 다른 분포를 띠고있는 데이터를 말함.  \n",
    "매우 다르다는 것에 대한 정의는 각자 다를 수 있다.  \n",
    "\n",
    "**일반적으로 모든 값들이 대체적으로 Q1 - 1.5IQR ~ Q3 + 1.5IQR 사이에있는 데이터이면 아웃라이어가 없다고 말 할 수 있다.**\n",
    "\n",
    "IQR은 상위 쿼타일(75%)와 하위 쿼타일(25%)의 차로 정의 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "approxQuantile()함수를 사용해서 이상치 값을 확인 할 수 있다.\n",
    "- 첫번째 : 칼럼명\n",
    "- 두번째 : 0과 1사이 값 (0.5 중간 값)\n",
    "- 세번째 : 각 피처에 대한 허용 가능한 수준의 에러(0으로 지정시 피처에 대한 정확한 값을 계산 할 수 있으나 매우 많은 연산을 수행한다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['weight', 'height', 'age']\n",
    "bounds = {}\n",
    "for col in cols:\n",
    "    quantiles = df_outliers.approxQuantile(col, [0.25, 0.75], 0.05)\n",
    "    IQR = quantiles[1] - quantiles[0]\n",
    "    bounds[col] = [quantiles[0] - 1.5 * IQR, quantiles[1] + 1.5 * IQR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': [72.19999999999999, 224.2],\n",
       " 'height': [4.15, 6.950000000000001],\n",
       " 'age': [1.5, 85.5]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = df_outliers.select(\n",
    "    *['id'] + \n",
    "    [(\n",
    "        (df_outliers[c] < bounds[c][0]) |\n",
    "        (df_outliers[c] > bounds[c][1])\n",
    "     ).alias(c + '_o') for c in cols ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-----+\n",
      "| id|weight_o|height_o|age_o|\n",
      "+---+--------+--------+-----+\n",
      "|  1|   false|   false|false|\n",
      "|  2|   false|   false|false|\n",
      "|  3|    true|   false| true|\n",
      "|  4|   false|   false|false|\n",
      "|  5|   false|   false|false|\n",
      "|  6|   false|   false|false|\n",
      "|  7|   false|   false|false|\n",
      "+---+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outliers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers = df_outliers.join(outliers, on='id')\n",
    "df_outliers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers.filter('weight_o').select('id', 'weight').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers.filter('age_o').select('id', 'age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand data\n",
    "기술 통계 : 기술 통계는 데이터셋에서의 관찰 값 갯수, 각 컬럼의 평균, 표준 편차 또는 최댓값, 최솟값 등의 기본 적인 정보를 확인 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = sc.textFile('./ccFraud.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"custID\",\"gender\",\"state\",\"cardholder\",\"balance\",\"numTrans\",\"numIntlTrans\",\"creditLine\",\"fraudRisk\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = fraud.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = fraud.filter(lambda row: row != header) \\\n",
    "             .map(lambda row: [int(elem) for elem in row.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    *[\n",
    "        typ.StructField(h[1:-1], typ.IntegerType(), True) for h in header.split(',')\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(custID,IntegerType,true),\n",
       " StructField(gender,IntegerType,true),\n",
       " StructField(state,IntegerType,true),\n",
       " StructField(cardholder,IntegerType,true),\n",
       " StructField(balance,IntegerType,true),\n",
       " StructField(numTrans,IntegerType,true),\n",
       " StructField(numIntlTrans,IntegerType,true),\n",
       " StructField(creditLine,IntegerType,true),\n",
       " StructField(fraudRisk,IntegerType,true)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = typ.StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df = spark.createDataFrame(fraud, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- custID: integer (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- state: integer (nullable = true)\n",
      " |-- cardholder: integer (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- numTrans: integer (nullable = true)\n",
      " |-- numIntlTrans: integer (nullable = true)\n",
      " |-- creditLine: integer (nullable = true)\n",
      " |-- fraudRisk: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fraud_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|gender|  count|\n",
      "+------+-------+\n",
      "|     1|6178231|\n",
      "|     2|3821769|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 빈도수 구하기\n",
    "fraud_df.groupby('gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descrive() 확인\n",
    "numerical = ['balance', 'numTrans', 'numIntlTrans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = fraud_df.describe(numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+\n",
      "|summary|          balance|          numTrans|     numIntlTrans|\n",
      "+-------+-----------------+------------------+-----------------+\n",
      "|  count|         10000000|          10000000|         10000000|\n",
      "|   mean|     4109.9199193|        28.9351871|        4.0471899|\n",
      "| stddev|3996.847309737077|26.553781024522852|8.602970115863767|\n",
      "|    min|                0|                 0|                0|\n",
      "|    max|            41485|               100|               60|\n",
      "+-------+-----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "desc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기술 통계는 적은 정보이지만 많은 것들을 알 수 있다.  \n",
    "- 모든 피처는 양의 방향으로 왜곡이 돼었다 (최대값이 평균보다 몇배 더 크다)\n",
    "- 변동계수 COEFFICIENT VARIABIOIN 가 매우 크다 (값이 1과 가깝거나 크다, 이는 넓게 퍼진 데이터를 의미)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "| skewness(balance)|\n",
      "+------------------+\n",
      "|1.1818315552995033|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 비대칭 확인\n",
    "fraud_df.agg({'balance': 'skewness'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00044523140172659576"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlations\n",
    "# corr() 함수는 두쌍의 상관계수만 계산 할 수 있다, 피어슨 상관계수를 지원한다.\n",
    "fraud_df.corr('balance', 'numTrans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_numerical = len(numerical)\n",
    "corr = []\n",
    "for i in range(0, n_numerical):\n",
    "    temp = [None] * i\n",
    "    for j in range(i, n_numerical):\n",
    "        temp.append(fraud_df.corr(numerical[i], numerical[j]))\n",
    "    corr.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 0.00044523140172659576, 0.00027139913398184604],\n",
       " [None, 1.0, -0.0002805712819816179],\n",
       " [None, None, 1.0]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'balance','numTrans', 'numIntlTrans'\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at C:\\Users\\a\\Anaconda3:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n"
     ]
    }
   ],
   "source": [
    "!conda list holoviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at C:\\Users\\a\\Anaconda3:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "bkcharts                  0.2                      py37_0  \n"
     ]
    }
   ],
   "source": [
    "!conda list bkcharts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: ...working... failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\a\\Anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - holoviews\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    holoviews-1.12.7           |     pyh9f0ad1d_1         3.2 MB  conda-forge\n",
      "    matplotlib-base-3.2.1      |   py37h911224e_0         7.1 MB  conda-forge\n",
      "    param-1.9.3                |             py_0          60 KB  conda-forge\n",
      "    pyviz_comms-0.7.6          |     pyh9f0ad1d_0          13 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        10.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  holoviews          conda-forge/noarch::holoviews-1.12.7-pyh9f0ad1d_1\n",
      "  matplotlib-base    conda-forge/win-64::matplotlib-base-3.2.1-py37h911224e_0\n",
      "  param              conda-forge/noarch::param-1.9.3-py_0\n",
      "  pyviz_comms        conda-forge/noarch::pyviz_comms-0.7.6-pyh9f0ad1d_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "holoviews-1.12.7     | 3.2 MB    |            |   0% \n",
      "holoviews-1.12.7     | 3.2 MB    |            |   0% \n",
      "holoviews-1.12.7     | 3.2 MB    | 4          |   4% \n",
      "holoviews-1.12.7     | 3.2 MB    | 8          |   8% \n",
      "holoviews-1.12.7     | 3.2 MB    | #5         |  15% \n",
      "holoviews-1.12.7     | 3.2 MB    | ##5        |  25% \n",
      "holoviews-1.12.7     | 3.2 MB    | ###5       |  36% \n",
      "holoviews-1.12.7     | 3.2 MB    | #####4     |  54% \n",
      "holoviews-1.12.7     | 3.2 MB    | #######6   |  77% \n",
      "holoviews-1.12.7     | 3.2 MB    | ########## | 100% \n",
      "holoviews-1.12.7     | 3.2 MB    | ########## | 100% \n",
      "\n",
      "matplotlib-base-3.2. | 7.1 MB    |            |   0% \n",
      "matplotlib-base-3.2. | 7.1 MB    | 4          |   4% \n",
      "matplotlib-base-3.2. | 7.1 MB    | ##4        |  25% \n",
      "matplotlib-base-3.2. | 7.1 MB    | ####1      |  41% \n",
      "matplotlib-base-3.2. | 7.1 MB    | #####9     |  60% \n",
      "matplotlib-base-3.2. | 7.1 MB    | #######8   |  78% \n",
      "matplotlib-base-3.2. | 7.1 MB    | #########6 |  96% \n",
      "matplotlib-base-3.2. | 7.1 MB    | ########## | 100% \n",
      "\n",
      "pyviz_comms-0.7.6    | 13 KB     |            |   0% \n",
      "pyviz_comms-0.7.6    | 13 KB     | ########## | 100% \n",
      "\n",
      "param-1.9.3          | 60 KB     |            |   0% \n",
      "param-1.9.3          | 60 KB     | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge holoviews -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "히스토그램은 피처의 분포를 시각화하는 가장 쉬운 방법이다.  \n",
    "스파크에서 히스토그램을 만드는 방법은 3가지이다.\n",
    "- 데이터를 워커노드에 집계해서 워커노드가 bin리스트를 드라이버 노드에게 리턴하고 각 bin의 개수를 드라이버 노드가 카운트\n",
    "- 데이터를 모두 드라이버 노드에 리턴하고 시각화 라이브러리 함수를 사용해 히스토그램을 만듬.\n",
    "- 데이터를 샘플링해 드라이버 노드에 리턴, 리턴된 데이터를 이용해 시각화 한다.\n",
    "(데이터셋의 행수가 너무 많으면 두번째 방법은 작업이 불가능하다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 워커 노드에서 집계해서 워커 노드가 bin 리스트를 드라이버 노드에게 리턴하고 각 bin의개수를 드라이버 노드가 카운트\n",
    "hist = fraud_df.select('balance').rdd.flatMap(lambda row: row).histogram(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\a\\Anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - matplotlib\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-4.8.5                |           py37_0         3.1 MB  anaconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.1 MB\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  conda              conda-forge::conda-4.8.5-py37hc8dfbb8~ --> anaconda::conda-4.8.5-py37_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "conda-4.8.5          | 3.1 MB    |            |   0% \n",
      "conda-4.8.5          | 3.1 MB    |            |   1% \n",
      "conda-4.8.5          | 3.1 MB    | #8         |  18% \n",
      "conda-4.8.5          | 3.1 MB    | ###5       |  35% \n",
      "conda-4.8.5          | 3.1 MB    | #####3     |  53% \n",
      "conda-4.8.5          | 3.1 MB    | #######5   |  76% \n",
      "conda-4.8.5          | 3.1 MB    | ########## | 100% \n",
      "conda-4.8.5          | 3.1 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c anaconda matplotlib -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'bins': hist[0][:-1],\n",
    "    'freq': hist[1]\n",
    "}\n",
    "fig = plt.figure(figsize=(12, 0))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.bar(data['bins'], data['freq'], width=2000)\n",
    "ax.set_title('Histogram of \\'balance\\'')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Bars(data['freq'], 'freq', 'bins') + hv.Bars(data['bins'], 'freq', 'bins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터의 양이 드라이버 노드에서 처리 할 수 있을 정도로 충분히 작다면,\n",
    "# 데이터를 드라이버 노드로 가져와서 matplotlib의 hist() 함수 또는 Bokeh의 Histoghram() 함수를 사용해\n",
    "# 히스토그램을 작성 할 수 있다.\n",
    "\n",
    "data_driver = {'obs': fraud_df.select('balance').rdd.flatMap(lambda row: row).collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_driver['obs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 9))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.hist(data_driver['obs'], bins=20)\n",
    "ax.set_title('Histogram of \\'balance\\' using .hist()')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter \n",
    "변수간의 상호작용을 시각화 할 수 있다.\n",
    "스파크에서는 어떠한 시각화 모듈도 제공하지 않는다.  \n",
    "수십업개의 데이터를 동시에 시각화 하는것은 비현식적이다.  \n",
    "데이터셋에서 0.02%로 셈플링하여 시각화를 해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = fraud_df.sampleBy('gender', {0: 0.0002, 1: 0.0002}).select(numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- numTrans: integer (nullable = true)\n",
      " |-- numIntlTrans: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_multi = dict([\n",
    "    (elem, data_sample.select(elem).rdd.flatMap(lambda row:row).collect()) for elem in numerical\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Scatter(data_multi, x='balance', y='numTrans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
