{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = SparkConf().setAppName('appName3').setMaster('local')\n",
    "# conf=SparkConf()\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "conf.set(\"spark.driver.memory\", \"4g\")\n",
    "conf.set(\"spark.cores.max\", \"4\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML package of Pyspark\n",
    "스파크 2.0부터 ML 패키지는 데이터프레임에 대해 작동  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ\n",
    "labels = [\n",
    "    ('INFANT_ALIVE_AT_REPORT', typ.IntegerType()),\n",
    "    ('BIRTH_PLACE', typ.IntegerType()),\n",
    "    ('MOTHER_AGE_YEARS', typ.IntegerType()),\n",
    "    ('FATHER_COMBINED_AGE', typ.IntegerType()),\n",
    "    ('CIG_BEFORE', typ.IntegerType()),\n",
    "    ('CIG_1_TRI', typ.IntegerType()),\n",
    "    ('CIG_2_TRI', typ.IntegerType()),\n",
    "    ('CIG_3_TRI', typ.IntegerType()),\n",
    "    ('MOTHER_HEIGHT_IN', typ.IntegerType()),\n",
    "    ('MOTHER_PRE_WEIGHT', typ.IntegerType()),\n",
    "    ('MOTHER_DELIVERY_WEIGHT', typ.IntegerType()),\n",
    "    ('MOTHER_WEIGHT_GAIN', typ.IntegerType()),\n",
    "    ('DIABETES_PRE', typ.IntegerType()),\n",
    "    ('DIABETES_GEST', typ.IntegerType()),\n",
    "    ('HYP_TENS_PRE', typ.IntegerType()),\n",
    "    ('HYP_TENS_GEST', typ.IntegerType()),\n",
    "    ('INFANT_WEIGHT_GRAMS', typ.IntegerType())\n",
    "]\n",
    "\n",
    "schema = typ.StructType([typ.StructField(e[0], e[1], False) for e in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "births = spark.read.csv('births_transformed.csv.gz', header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIRTH_PLACE 칼럼 인코딩 작업\n",
    "births = births.withColumn('BIRTH_PLACE_INT', births['BIRTH_PLACE'].cast(typ.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.feature as ft\n",
    "encoder = ft.OneHotEncoder(inputCol = 'BIRTH_PLACE_INT', outputCol='BIRTH_PLACE_VEC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCreator = ft.VectorAssembler(inputCols=[col[0] for col in labels[2:]] + [encoder.getOutputCol()], \n",
    "                                     outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.classification as cl\n",
    "logistic = cl.LogisticRegression(maxIter=10, regParam=0.01, \n",
    "                                 labelCol='INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipline = Pipeline(stages=[encoder, featuresCreator, logistic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "births_train, births_test = births.randomSplit([0.7, 0.3], seed=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipline.fit(births_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = model.transform(births_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(INFANT_ALIVE_AT_REPORT=0, BIRTH_PLACE=1, MOTHER_AGE_YEARS=12, FATHER_COMBINED_AGE=99, CIG_BEFORE=0, CIG_1_TRI=0, CIG_2_TRI=0, CIG_3_TRI=0, MOTHER_HEIGHT_IN=62, MOTHER_PRE_WEIGHT=145, MOTHER_DELIVERY_WEIGHT=152, MOTHER_WEIGHT_GAIN=7, DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, INFANT_WEIGHT_GRAMS=0, BIRTH_PLACE_INT=1, BIRTH_PLACE_VEC=SparseVector(9, {1: 1.0}), features=SparseVector(24, {0: 12.0, 1: 99.0, 6: 62.0, 7: 145.0, 8: 152.0, 9: 7.0, 16: 1.0}), rawPrediction=DenseVector([0.9458, -0.9458]), probability=DenseVector([0.7203, 0.2797]), prediction=0.0)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.evaluation as ev\n",
    "evaluator = ev.BinaryClassificationEvaluator( rawPredictionCol='probability', labelCol='INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7434490669849834\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(test_model, {evaluator.metricName: 'areaUnderROC'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7201551931757337\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(test_model, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "piplinePath = './infant_oneHotEncoder_logistic_pipline'\n",
    "pipline.write().overwrite().save(piplinePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedPipline = Pipeline.load(piplinePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(INFANT_ALIVE_AT_REPORT=0, BIRTH_PLACE=1, MOTHER_AGE_YEARS=12, FATHER_COMBINED_AGE=99, CIG_BEFORE=0, CIG_1_TRI=0, CIG_2_TRI=0, CIG_3_TRI=0, MOTHER_HEIGHT_IN=62, MOTHER_PRE_WEIGHT=145, MOTHER_DELIVERY_WEIGHT=152, MOTHER_WEIGHT_GAIN=7, DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, INFANT_WEIGHT_GRAMS=0, BIRTH_PLACE_INT=1, BIRTH_PLACE_VEC=SparseVector(9, {1: 1.0}), features=SparseVector(24, {0: 12.0, 1: 99.0, 6: 62.0, 7: 145.0, 8: 152.0, 9: 7.0, 16: 1.0}), rawPrediction=DenseVector([0.9458, -0.9458]), probability=DenseVector([0.7203, 0.2797]), prediction=0.0)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadedPipline.fit(births_train).transform(births_test).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "modelPath = './infant_oneHotEncoder_logistic_piplineModel'\n",
    "model.write().overwrite().save(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedPiplineModel = PipelineModel.load(modelPath)\n",
    "test_loadedModel = loadedPiplineModel.transform(births_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter hyper-tuning\n",
    "Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = cl.LogisticRegression(labelCol='INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = tune.ParamGridBuilder().addGrid(logistic.maxIter, [2, 10, 50])\\\n",
    "                              .addGrid(logistic.regParam, [0.01, 0.05, 0.3])\\\n",
    "                              .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='probability', labelCol='INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = tune.CrossValidator(estimator=logistic, estimatorParamMaps=grid, evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipline = Pipeline(stages=[encoder, featuresCreator])\n",
    "data_transformer = pipline.fit(births_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(data_transformer.transform(births_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_transformer.transform(births_test)\n",
    "results = cvModel.transform(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7441328157027877\n",
      "0.7212737531230229\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(results, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(results, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    ([{key.name: paramValue} for key, paramValue in zip(params.keys(), params.values())], metric)\n",
    "    for params, metric in zip(cvModel.getEstimatorParamMaps(), cvModel.avgMetrics)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(results, key=lambda el: el[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ft.ChiSqSelector(numTopFeatures=5,\n",
    "                           featuresCol=featuresCreator.getOutputCol(),\n",
    "                           outputCol='selectedFeatures',\n",
    "                           labelCol='INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = cl.LogisticRegression(labelCol='INFANT_ALIVE_AT_REPORT',\n",
    "                                featuresCol='selectedFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipline = Pipeline(stages=[encoder, featuresCreator, selector])\n",
    "data_transformer = pipline.fit(births_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs = tune.TrainValidationSplit(estimator=logistic, estimatorParamMaps=grid, evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvsModel = tvs.fit(data_transformer.transform(births_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_transformer.transform(births_test)\n",
    "results = tvsModel.transform(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.735357446903953\n",
      "0.7148733525428738\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(results, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(results, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준화 작업\n",
    "import numpy as np\n",
    "x = np.arange(0, 100)\n",
    "x = x / 100.0 * np.pi * 4\n",
    "y = x * np.sin(x / 1.764) + 20.1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = typ.StructType([\n",
    "    typ.StructField('continuous_var', typ.DoubleType(), False)\n",
    "])\n",
    "data = spark.createDataFrame([[float(e), ] for e in y], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|    continuous_var|\n",
      "+------------------+\n",
      "|           20.1234|\n",
      "|20.132344452369832|\n",
      "|20.159087064491775|\n",
      "|20.203356291885854|\n",
      "| 20.26470185735763|\n",
      "|20.342498180090526|\n",
      "|  20.4359491438498|\n",
      "|20.544094172020312|\n",
      "|20.665815568330437|\n",
      "|20.799847073505322|\n",
      "|  20.9447835797997|\n",
      "| 21.09909193743627|\n",
      "|21.261122779470593|\n",
      "| 21.42912328456607|\n",
      "| 21.60125079063745|\n",
      "|21.775587166351258|\n",
      "|21.950153842094366|\n",
      "|22.122927397273514|\n",
      "|22.291855596719525|\n",
      "|22.454873765567744|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = ft.VectorAssembler(inputCols=['continuous_var'], outputCol='continuous_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = ft.StandardScaler(inputCol=vectorizer.getOutputCol(),\n",
    "                              outputCol='normalized',\n",
    "                              withMean=True,\n",
    "                              withStd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipline = Pipeline(stages=[vectorizer, normalizer])\n",
    "data_standardized = pipline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_standardized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
