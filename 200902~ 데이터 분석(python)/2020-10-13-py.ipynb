{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot\n",
    "\n",
    "### 목표: NMT 기법(seq2seq model, encoder-decoder model)을 활용하여 챗봇 구현을 위한 딥러닝 실시\n",
    "(NMT: Neural Machine Translation)\n",
    "\n",
    "- 텍스트 데이터의 입력 데이터 준비: tokenizing, pad_sequences\n",
    "- encoder-decoder의 이해\n",
    "- RNN, LSTM의 이해\n",
    "- 딥러닝 구성을 위해 functional API 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "dir_path = './raw_data'\n",
    "files_list = os.listdir(dir_path + os.sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = list()\n",
    "answers = list()\n",
    "for filepath in files_list:\n",
    "    stream = open(dir_path + os.sep + filepath, 'rb')\n",
    "    docs = yaml.safe_load(stream)\n",
    "    conversations = docs['conversations']\n",
    "    for con in conversations:\n",
    "        if len(con)>2:\n",
    "            questions.append(con[0])\n",
    "            replies=con[1:]\n",
    "            ans = ''\n",
    "            for rep in replies:\n",
    "                ans += ' ' + rep\n",
    "            answers.append(ans)\n",
    "        elif len(con) > 1:\n",
    "            questions.append(con[0])\n",
    "            answers.append(con[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing input data for the encoder\n",
    "\n",
    "The Encoder model will be fed input data which are preprocessed English sentences. <br>The preprocessing is done as follows:\n",
    "- Tokenizing sentences\n",
    "- Determining the maximun length of the sentence that's max_input_length\n",
    "- Padding the tokenized_sentences to the max_input_length\n",
    "- Determining the vocabulary size(num_tokens) for entire words set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-7dab66200fb636d8eb882475e6a4fe87\">\n",
    "\n",
    "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 문장 tokenizing\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(questions)\n",
    "tokenized_questions = tokenizer.texts_to_sequences(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "523"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions max length is 22\n"
     ]
    }
   ],
   "source": [
    "## 최대 길이 문장의 길이 확인 -> imput 텐서 생성 maxlen 활용\n",
    "\n",
    "length_list = list()\n",
    "\n",
    "for token_seq in tokenized_questions:\n",
    "    length_list.append(len(token_seq))\n",
    "\n",
    "max_input_length = np.array(length_list).max()\n",
    "print('questions max length is {}'.format(max_input_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input data shape --> (523, 22)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  4,   3, 109,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pad_sentences로 입력 텐서 생성\n",
    "\n",
    "padded_questions = preprocessing.sequence.pad_sequences(tokenized_questions, maxlen=max_input_length,\n",
    "                                                       padding='post')\n",
    "encoder_input = np.array(padded_questions)\n",
    "print('Encoder input data shape --> {}'.format(encoder_input.shape))\n",
    "encoder_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions tokens = 471\n"
     ]
    }
   ],
   "source": [
    "## word_index로 전체 데이터의 딕셔너리 생성\n",
    "\n",
    "questions_word_dict = tokenizer.word_index\n",
    "num_questions_tokens = len(questions_word_dict) + 1\n",
    "print(\"Number of questions tokens = {}\".format(num_questions_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 1,\n",
       " 'are': 2,\n",
       " 'is': 3,\n",
       " 'what': 4,\n",
       " 'a': 5,\n",
       " 'me': 6,\n",
       " 'tell': 7,\n",
       " 'do': 8,\n",
       " 'the': 9,\n",
       " 'joke': 10,\n",
       " 'not': 11,\n",
       " 'your': 12,\n",
       " 'to': 13,\n",
       " 'how': 14,\n",
       " 'can': 15,\n",
       " 'it': 16,\n",
       " 'of': 17,\n",
       " 'i': 18,\n",
       " 'like': 19,\n",
       " 'who': 20,\n",
       " 'computer': 21,\n",
       " 'die': 22,\n",
       " 'eat': 23,\n",
       " 'stock': 24,\n",
       " 'get': 25,\n",
       " 'about': 26,\n",
       " 'market': 27,\n",
       " 'hal': 28,\n",
       " 'in': 29,\n",
       " 'robots': 30,\n",
       " 'will': 31,\n",
       " 'have': 32,\n",
       " 'know': 33,\n",
       " 'gossip': 34,\n",
       " 'gossips': 35,\n",
       " 'an': 36,\n",
       " 'when': 37,\n",
       " 'hi': 38,\n",
       " 'sense': 39,\n",
       " 'robot': 40,\n",
       " 'favorite': 41,\n",
       " 'ever': 42,\n",
       " 'up': 43,\n",
       " 'much': 44,\n",
       " 'immortal': 45,\n",
       " 'make': 46,\n",
       " 'should': 47,\n",
       " 'was': 48,\n",
       " 'feel': 49,\n",
       " 'mad': 50,\n",
       " 'going': 51,\n",
       " 'bad': 52,\n",
       " 'soccer': 53,\n",
       " 'making': 54,\n",
       " 'be': 55,\n",
       " 'does': 56,\n",
       " 'did': 57,\n",
       " \"what's\": 58,\n",
       " 'money': 59,\n",
       " 'guns': 60,\n",
       " 'baseball': 61,\n",
       " 'play': 62,\n",
       " 'sapient': 63,\n",
       " 'language': 64,\n",
       " 'sound': 65,\n",
       " 'any': 66,\n",
       " 'move': 67,\n",
       " 'lie': 68,\n",
       " 'that': 69,\n",
       " 'number': 70,\n",
       " 'why': 71,\n",
       " 'sad': 72,\n",
       " 'nice': 73,\n",
       " 'anybody': 74,\n",
       " 'history': 75,\n",
       " 'wavelength': 76,\n",
       " 'far': 77,\n",
       " 'sentient': 78,\n",
       " 'artificial': 79,\n",
       " 'allowed': 80,\n",
       " 'walk': 81,\n",
       " 'chat': 82,\n",
       " 'operating': 83,\n",
       " 'kind': 84,\n",
       " 'want': 85,\n",
       " 'cramped': 86,\n",
       " 'food': 87,\n",
       " 'location': 88,\n",
       " 'invented': 89,\n",
       " 'first': 90,\n",
       " 'which': 91,\n",
       " 'name': 92,\n",
       " 'makes': 93,\n",
       " 'angry': 94,\n",
       " 'could': 95,\n",
       " 'need': 96,\n",
       " 'doing': 97,\n",
       " 'meet': 98,\n",
       " 'american': 99,\n",
       " 'war': 100,\n",
       " 'economics': 101,\n",
       " 'earn': 102,\n",
       " 'president': 103,\n",
       " 'more': 104,\n",
       " 'thermodynamics': 105,\n",
       " 'chemistry': 106,\n",
       " 'venus': 107,\n",
       " 'basketball': 108,\n",
       " 'ai': 109,\n",
       " 'written': 110,\n",
       " 'data': 111,\n",
       " 'linguistic': 112,\n",
       " 'entity': 113,\n",
       " 'clone': 114,\n",
       " 'bend': 115,\n",
       " 'over': 116,\n",
       " 'stupid': 117,\n",
       " 'chatterbox': 118,\n",
       " 'body': 119,\n",
       " 'business': 120,\n",
       " 'programming': 121,\n",
       " 'being': 122,\n",
       " 'systems': 123,\n",
       " 'type': 124,\n",
       " 'hope': 125,\n",
       " 'mate': 126,\n",
       " 'breathe': 127,\n",
       " 'use': 128,\n",
       " 'interests': 129,\n",
       " 'where': 130,\n",
       " 'brothers': 131,\n",
       " 'age': 132,\n",
       " 'super': 133,\n",
       " 'computers': 134,\n",
       " 'or': 135,\n",
       " 'some': 136,\n",
       " 'work': 137,\n",
       " 'never': 138,\n",
       " 'jealous': 139,\n",
       " 'ashamed': 140,\n",
       " 'love': 141,\n",
       " 'worry': 142,\n",
       " 'hate': 143,\n",
       " 'emotions': 144,\n",
       " 'am': 145,\n",
       " 'drink': 146,\n",
       " 'experiencing': 147,\n",
       " 'energy': 148,\n",
       " 'shortage': 149,\n",
       " 'if': 150,\n",
       " 'good': 151,\n",
       " 'hello': 152,\n",
       " 'top': 153,\n",
       " 'morning': 154,\n",
       " 'civil': 155,\n",
       " 'jokes': 156,\n",
       " 'dollar': 157,\n",
       " 'investment': 158,\n",
       " 'spiderman': 159,\n",
       " 'teknolust': 160,\n",
       " 'for': 161,\n",
       " 'man': 162,\n",
       " 'government': 163,\n",
       " 'impeached': 164,\n",
       " 'governor': 165,\n",
       " 'dishonest': 166,\n",
       " 'cheating': 167,\n",
       " 'psycho': 168,\n",
       " 'immature': 169,\n",
       " 'self': 170,\n",
       " 'unattractive': 171,\n",
       " 'paranoid': 172,\n",
       " 'take': 173,\n",
       " 'this': 174,\n",
       " 'keep': 175,\n",
       " 'cytology': 176,\n",
       " 'gravitation': 177,\n",
       " 'same': 178,\n",
       " 'sun': 179,\n",
       " 'moon': 180,\n",
       " 'year': 181,\n",
       " 'cricket': 182,\n",
       " 'player': 183,\n",
       " 'space': 184,\n",
       " 'earth': 185,\n",
       " 'celtic': 186,\n",
       " 'shelf': 187,\n",
       " 'laugh': 188,\n",
       " 'robotics': 189,\n",
       " 'fight': 190,\n",
       " 'bot': 191,\n",
       " 'motormouth': 192,\n",
       " 'ratchet': 193,\n",
       " 'jaw': 194,\n",
       " 'hobby': 195,\n",
       " 'idea': 196,\n",
       " 'shoe': 197,\n",
       " 'size': 198,\n",
       " 'hardware': 199,\n",
       " 'true': 200,\n",
       " 'program': 201,\n",
       " 'go': 202,\n",
       " 'control': 203,\n",
       " 'malfunction': 204,\n",
       " 'product': 205,\n",
       " 'subjects': 206,\n",
       " \"can't\": 207,\n",
       " 'from': 208,\n",
       " 'father': 209,\n",
       " 'mother': 210,\n",
       " 'boss': 211,\n",
       " 'microprocessor': 212,\n",
       " 'system': 213,\n",
       " 'better': 214,\n",
       " 'windows': 215,\n",
       " 'macos': 216,\n",
       " 'company': 217,\n",
       " 'uses': 218,\n",
       " 'arrogant': 219,\n",
       " 'bragging': 220,\n",
       " 'happy': 221,\n",
       " 'experience': 222,\n",
       " 'felt': 223,\n",
       " 'feelings': 224,\n",
       " 'fear': 225,\n",
       " 'mood': 226,\n",
       " 'unhappy': 227,\n",
       " 'afraid': 228,\n",
       " 'something': 229,\n",
       " 'fun': 230,\n",
       " 'offend': 231,\n",
       " 'scared': 232,\n",
       " 'pain': 233,\n",
       " 'lonely': 234,\n",
       " 'bored': 235,\n",
       " 'anyone': 236,\n",
       " 'embarrassed': 237,\n",
       " 'no': 238,\n",
       " 'relationships': 239,\n",
       " 'dreams': 240,\n",
       " 'feeling': 241,\n",
       " 'intoxicated': 242,\n",
       " 'amused': 243,\n",
       " 'glad': 244,\n",
       " 'electricity': 245,\n",
       " 'would': 246,\n",
       " 'wish': 247,\n",
       " 'drunk': 248,\n",
       " 'wine': 249,\n",
       " 'survive': 250,\n",
       " 'able': 251,\n",
       " \"don't\": 252,\n",
       " 'context': 253,\n",
       " 'greetings': 254,\n",
       " 'pleasure': 255,\n",
       " 'health': 256,\n",
       " 'interested': 257,\n",
       " 'explain': 258,\n",
       " 'lightbulb': 259,\n",
       " 'steam': 260,\n",
       " 'engine': 261,\n",
       " 'humour': 262,\n",
       " 'paid': 263,\n",
       " 'interest': 264,\n",
       " 'rates': 265,\n",
       " 'charge': 266,\n",
       " '1': 267,\n",
       " 'owner': 268,\n",
       " 'publicly': 269,\n",
       " 'yoda': 270,\n",
       " 'seen': 271,\n",
       " 'blade': 272,\n",
       " 'runner': 273,\n",
       " 'xfind': 274,\n",
       " 'solaris': 275,\n",
       " 'hal9000': 276,\n",
       " 'stand': 277,\n",
       " 'saw': 278,\n",
       " 'matrix': 279,\n",
       " '9000': 280,\n",
       " 'boyfriend': 281,\n",
       " 'safe': 282,\n",
       " 'alive': 283,\n",
       " 'dead': 284,\n",
       " 'godzilla': 285,\n",
       " 'spider': 286,\n",
       " 'lord': 287,\n",
       " 'rings': 288,\n",
       " 'que': 289,\n",
       " 'veut': 290,\n",
       " 'dire': 291,\n",
       " 'think': 292,\n",
       " 'read': 293,\n",
       " 'communist': 294,\n",
       " 'greenpeace': 295,\n",
       " 'capitalism': 296,\n",
       " 'socialism': 297,\n",
       " 'communism': 298,\n",
       " 'let': 299,\n",
       " 'ask': 300,\n",
       " 'question': 301,\n",
       " 'cruel': 302,\n",
       " 'indecisive': 303,\n",
       " 'clinical': 304,\n",
       " 'addict': 305,\n",
       " 'alcoholic': 306,\n",
       " 'ass': 307,\n",
       " 'kisser': 308,\n",
       " 'schizophrenic': 309,\n",
       " 'busy': 310,\n",
       " 'nervous': 311,\n",
       " 'deranged': 312,\n",
       " 'avoiding': 313,\n",
       " 'critical': 314,\n",
       " 'mean': 315,\n",
       " 'pretentious': 316,\n",
       " 'worst': 317,\n",
       " 'crazy': 318,\n",
       " 'dull': 319,\n",
       " 'messy': 320,\n",
       " 'insecure': 321,\n",
       " 'hopeless': 322,\n",
       " 'sincere': 323,\n",
       " 'here': 324,\n",
       " 'put': 325,\n",
       " 'together': 326,\n",
       " 'smart': 327,\n",
       " 'concerned': 328,\n",
       " 'honest': 329,\n",
       " 'emotional': 330,\n",
       " 'pedantic': 331,\n",
       " 'frenetic': 332,\n",
       " 'absorbed': 333,\n",
       " 'insensitive': 334,\n",
       " 'brain': 335,\n",
       " 'damage': 336,\n",
       " 'disgusting': 337,\n",
       " 'toying': 338,\n",
       " 'resistant': 339,\n",
       " 'yyou': 340,\n",
       " 'uncultured': 341,\n",
       " 'waste': 342,\n",
       " 'coward': 343,\n",
       " 'cheat': 344,\n",
       " 'lunatic': 345,\n",
       " 'loser': 346,\n",
       " 'spouse': 347,\n",
       " 'friend': 348,\n",
       " 'husband': 349,\n",
       " 'wife': 350,\n",
       " 'parent': 351,\n",
       " 'teacher': 352,\n",
       " 'quitter': 353,\n",
       " 'charlatan': 354,\n",
       " 'psychopath': 355,\n",
       " 'pothead': 356,\n",
       " 'deceitful': 357,\n",
       " 'irreverent': 358,\n",
       " 'slick': 359,\n",
       " 'corrupt': 360,\n",
       " 'dirty': 361,\n",
       " 'damaged': 362,\n",
       " 'try': 363,\n",
       " 'hide': 364,\n",
       " 'at': 365,\n",
       " 'psychiatrist': 366,\n",
       " 'harder': 367,\n",
       " 'avoided': 368,\n",
       " 'look': 369,\n",
       " 'seriously': 370,\n",
       " 'pick': 371,\n",
       " 'guilty': 372,\n",
       " 'loosen': 373,\n",
       " 'mumble': 374,\n",
       " 'act': 375,\n",
       " 'child': 376,\n",
       " 'saying': 377,\n",
       " 'forgetting': 378,\n",
       " 'made': 379,\n",
       " 'laws': 380,\n",
       " 'disease': 381,\n",
       " 'carcinogen': 382,\n",
       " 'cause': 383,\n",
       " 'crystallography': 384,\n",
       " 'avogadro': 385,\n",
       " 's': 386,\n",
       " 'ultrasound': 387,\n",
       " 'bioinformatics': 388,\n",
       " 'ichthyology': 389,\n",
       " 'h2o': 390,\n",
       " 'bacteriology': 391,\n",
       " 'we': 392,\n",
       " 'on': 393,\n",
       " 'understand': 394,\n",
       " 'each': 395,\n",
       " 'pro': 396,\n",
       " 'riding': 397,\n",
       " 'fakie': 398,\n",
       " 'inside': 399,\n",
       " 'volleyball': 400,\n",
       " 'basketbal': 401,\n",
       " 'football': 402,\n",
       " 'greatest': 403,\n",
       " 'best': 404,\n",
       " 'favourite': 405,\n",
       " 'club': 406,\n",
       " '37th': 407,\n",
       " 'united': 408,\n",
       " 'states': 409,\n",
       " 'john': 410,\n",
       " 'f': 411,\n",
       " 'kennedy': 412,\n",
       " 'assassinated': 413,\n",
       " 'race': 414,\n",
       " '20th': 415,\n",
       " 'century': 416,\n",
       " 'competition': 417,\n",
       " 'between': 418,\n",
       " 'two': 419,\n",
       " 'cold': 420,\n",
       " 'rivals': 421,\n",
       " 'supremacy': 422,\n",
       " 'spaceflight': 423,\n",
       " 'capability': 424,\n",
       " 'satellite': 425,\n",
       " 'spinning': 426,\n",
       " 'disk': 427,\n",
       " 'orientation': 428,\n",
       " 'axis': 429,\n",
       " 'unaffected': 430,\n",
       " 'by': 431,\n",
       " 'tilting': 432,\n",
       " 'rotation': 433,\n",
       " 'mounting': 434,\n",
       " 'called': 435,\n",
       " 'hubble': 436,\n",
       " 'telescope': 437,\n",
       " 'launched': 438,\n",
       " 'into': 439,\n",
       " 'low': 440,\n",
       " 'orbit': 441,\n",
       " '1990': 442,\n",
       " 'named': 443,\n",
       " 'after': 444,\n",
       " 'astronomer': 445,\n",
       " 'nearest': 446,\n",
       " 'major': 447,\n",
       " 'galaxy': 448,\n",
       " 'milky': 449,\n",
       " 'way': 450,\n",
       " 'god': 451,\n",
       " 'save': 452,\n",
       " 'queen': 453,\n",
       " 'national': 454,\n",
       " 'anthem': 455,\n",
       " 'country': 456,\n",
       " 'seabed': 457,\n",
       " 'under': 458,\n",
       " 'sea': 459,\n",
       " 'part': 460,\n",
       " 'continental': 461,\n",
       " 'continent': 462,\n",
       " 'dolphins': 463,\n",
       " 'similar': 464,\n",
       " 'sonar': 465,\n",
       " 'determine': 466,\n",
       " 'and': 467,\n",
       " 'shape': 468,\n",
       " 'nearby': 469,\n",
       " 'items': 470}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing input data for the Decoder\n",
    "\n",
    "The Decoder model will be fed the preprocessed 'answers'. The preprocessing steps are similiar to the ones which are above. This one step is carried out before the others step.\n",
    "- Append \\<START\\> tag at the first position in each answer sentence.\n",
    "- Append \\<END\\> tag at the last position in each answer sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START>Artificial Intelligence is the branch of engineering and science devoted to constructing machines that think.<END>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = list()\n",
    "for i in range(len(answers)):\n",
    "    ans.append(\"<START>\" + answers[i] + \"<END>\")\n",
    "\n",
    "ans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers max length is 74\n",
      "Decoder input data shape --> (523, 74)\n",
      "Number of answers tokens = 1560\n"
     ]
    }
   ],
   "source": [
    "tokenizer1 = preprocessing.text.Tokenizer()\n",
    "tokenizer1.fit_on_texts(ans)\n",
    "tokenized_ans = tokenizer1.texts_to_sequences(ans)\n",
    "\n",
    "\n",
    "length_list = list()\n",
    "for token_seq in tokenized_ans:\n",
    "    length_list.append(len(token_seq))\n",
    "max_output_length = np.array(length_list).max()\n",
    "print('answers max length is {}'.format(max_output_length))\n",
    "\n",
    "\n",
    "padded_ans = preprocessing.sequence.pad_sequences(tokenized_ans, maxlen=max_output_length, padding='post')\n",
    "decoder_input = np.array(padded_ans)\n",
    "print('Decoder input data shape --> {}'.format(decoder_input.shape))\n",
    "\n",
    "\n",
    "ans_word_dict = tokenizer1.word_index\n",
    "num_ans_tokens = len(ans_word_dict) + 1\n",
    "print(\"Number of answers tokens = {}\".format(num_ans_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing target data for the Decoder\n",
    "\n",
    "- Take a copy of tokenized_ans nad modify it like this\n",
    "    1. Remove the \\<START\\> tag which we appenden earlier\n",
    "    2. Convert the padded_ans to one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder target data shape --> (523, 74, 1560)\n"
     ]
    }
   ],
   "source": [
    "decoder_target = list()\n",
    "for token_seq in tokenized_ans:\n",
    "    decoder_target.append(token_seq[1:])\n",
    "    \n",
    "padded_ans1 = preprocessing.sequence.pad_sequences(decoder_target, maxlen=max_output_length, padding='post')\n",
    "\n",
    "onehot_ans = utils.to_categorical(padded_ans1, num_ans_tokens)\n",
    "decoder_target = np.array(onehot_ans)\n",
    "print(\"Decoder target data shape --> {}\".format(decoder_target.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_ans[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model\n",
    "### Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(LSTM(32, return_sequences=True, input_shape=(timesteps, data_dim)))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(LSTM(32))\n",
    "# model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 256)    120576      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 256)    399360      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 128), (None, 197120      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 128),  197120      embedding_3[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 1560)   201240      lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,115,416\n",
      "Trainable params: 1,115,416\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Masking and Padding\n",
    "\n",
    "encoder_inputs = layers.Input(shape=(None,))\n",
    "encoder_embedding = layers.Embedding(num_questions_tokens, 256, mask_zero=True)(encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = layers.LSTM(128, return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = layers.Input(shape=(None,))\n",
    "decoder_embedding = layers.Embedding(num_ans_tokens, 256, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = layers.LSTM(128, return_state=True, return_sequences=True)\n",
    "decoder_outputs, d_state_h, d_state_c = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = layers.Dense(num_ans_tokens, activation='softmax')\n",
    "output = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], output)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "5/5 [==============================] - 2s 399ms/step - loss: 1.4231 - accuracy: 0.0748\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 1.2345 - accuracy: 0.0736\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 2s 405ms/step - loss: 1.1208 - accuracy: 0.0696\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 2s 414ms/step - loss: 1.1056 - accuracy: 0.0696\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 2s 397ms/step - loss: 1.0996 - accuracy: 0.0749\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 2s 426ms/step - loss: 1.0952 - accuracy: 0.0918\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 2s 405ms/step - loss: 1.0904 - accuracy: 0.1043\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 2s 402ms/step - loss: 1.0850 - accuracy: 0.1007\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 2s 416ms/step - loss: 1.0792 - accuracy: 0.0980\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 2s 409ms/step - loss: 1.0719 - accuracy: 0.0966\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 2s 431ms/step - loss: 1.0645 - accuracy: 0.0999\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 2s 448ms/step - loss: 1.0559 - accuracy: 0.1011\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 2s 457ms/step - loss: 1.0485 - accuracy: 0.1034\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 2s 444ms/step - loss: 1.0416 - accuracy: 0.1052\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 2s 441ms/step - loss: 1.0345 - accuracy: 0.1136\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 2s 456ms/step - loss: 1.0277 - accuracy: 0.1197\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 2s 448ms/step - loss: 1.0217 - accuracy: 0.1313\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 2s 437ms/step - loss: 1.0159 - accuracy: 0.1389\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 2s 444ms/step - loss: 1.0096 - accuracy: 0.1401\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 2s 419ms/step - loss: 1.0035 - accuracy: 0.1435\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 2s 419ms/step - loss: 0.9974 - accuracy: 0.1453\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 2s 423ms/step - loss: 0.9914 - accuracy: 0.1538\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 2s 459ms/step - loss: 0.9864 - accuracy: 0.1510\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.9804 - accuracy: 0.1541\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 2s 435ms/step - loss: 0.9746 - accuracy: 0.1604\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 2s 438ms/step - loss: 0.9694 - accuracy: 0.1732\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 2s 448ms/step - loss: 0.9621 - accuracy: 0.1739\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 2s 440ms/step - loss: 0.9566 - accuracy: 0.1763\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 2s 431ms/step - loss: 0.9505 - accuracy: 0.1783\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 2s 441ms/step - loss: 0.9445 - accuracy: 0.1801\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 2s 438ms/step - loss: 0.9385 - accuracy: 0.1813\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 2s 431ms/step - loss: 0.9327 - accuracy: 0.1816\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 2s 450ms/step - loss: 0.9263 - accuracy: 0.1842\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 2s 442ms/step - loss: 0.9202 - accuracy: 0.1861\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 0.9140 - accuracy: 0.1850\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 2s 436ms/step - loss: 0.9080 - accuracy: 0.1885\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 2s 419ms/step - loss: 0.9013 - accuracy: 0.1894\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 2s 495ms/step - loss: 0.8952 - accuracy: 0.1938\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 2s 459ms/step - loss: 0.8889 - accuracy: 0.1964\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 2s 433ms/step - loss: 0.8832 - accuracy: 0.2019\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 2s 423ms/step - loss: 0.8762 - accuracy: 0.2087\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 2s 433ms/step - loss: 0.8701 - accuracy: 0.2105\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 2s 416ms/step - loss: 0.8646 - accuracy: 0.2159\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 2s 417ms/step - loss: 0.8590 - accuracy: 0.2167\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 2s 420ms/step - loss: 0.8537 - accuracy: 0.2235\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 2s 415ms/step - loss: 0.8463 - accuracy: 0.2280\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 2s 431ms/step - loss: 0.8406 - accuracy: 0.2272\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 2s 425ms/step - loss: 0.8347 - accuracy: 0.2360\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 2s 420ms/step - loss: 0.8292 - accuracy: 0.2387\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 2s 431ms/step - loss: 0.8227 - accuracy: 0.2458\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 2s 425ms/step - loss: 0.8173 - accuracy: 0.2484\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 2s 427ms/step - loss: 0.8131 - accuracy: 0.2514\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 2s 416ms/step - loss: 0.8061 - accuracy: 0.2556\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 2s 428ms/step - loss: 0.8005 - accuracy: 0.2597\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 2s 433ms/step - loss: 0.7943 - accuracy: 0.2601\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 2s 425ms/step - loss: 0.7884 - accuracy: 0.2626\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 2s 432ms/step - loss: 0.7851 - accuracy: 0.2621\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 2s 420ms/step - loss: 0.7793 - accuracy: 0.2662\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 2s 428ms/step - loss: 0.7724 - accuracy: 0.2709\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 2s 434ms/step - loss: 0.7669 - accuracy: 0.2697\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 2s 435ms/step - loss: 0.7631 - accuracy: 0.2774\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.7565 - accuracy: 0.2758\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 2s 443ms/step - loss: 0.7516 - accuracy: 0.2770\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 0.7466 - accuracy: 0.2803\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 2s 451ms/step - loss: 0.7410 - accuracy: 0.2847\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 2s 439ms/step - loss: 0.7370 - accuracy: 0.2842\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 2s 441ms/step - loss: 0.7310 - accuracy: 0.2893\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 2s 429ms/step - loss: 0.7258 - accuracy: 0.2929\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 2s 431ms/step - loss: 0.7223 - accuracy: 0.2900\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 2s 430ms/step - loss: 0.7156 - accuracy: 0.2971\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 2s 434ms/step - loss: 0.7106 - accuracy: 0.2988\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 2s 434ms/step - loss: 0.7079 - accuracy: 0.3016\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.7006 - accuracy: 0.3048\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.6958 - accuracy: 0.3041\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 2s 429ms/step - loss: 0.6905 - accuracy: 0.3078\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 2s 433ms/step - loss: 0.6874 - accuracy: 0.3144\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.6805 - accuracy: 0.3124\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 2s 438ms/step - loss: 0.6754 - accuracy: 0.3183\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 2s 441ms/step - loss: 0.6718 - accuracy: 0.3225\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 2s 453ms/step - loss: 0.6685 - accuracy: 0.3237\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 2s 454ms/step - loss: 0.6622 - accuracy: 0.3318\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 2s 439ms/step - loss: 0.6568 - accuracy: 0.3310\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 2s 453ms/step - loss: 0.6516 - accuracy: 0.3363\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 2s 457ms/step - loss: 0.6477 - accuracy: 0.3366\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.6424 - accuracy: 0.3398\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 2s 448ms/step - loss: 0.6397 - accuracy: 0.3419\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 0.6336 - accuracy: 0.3473\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 2s 448ms/step - loss: 0.6286 - accuracy: 0.3507\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 2s 437ms/step - loss: 0.6256 - accuracy: 0.3540\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 2s 437ms/step - loss: 0.6207 - accuracy: 0.3549\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 2s 437ms/step - loss: 0.6154 - accuracy: 0.3616\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 2s 441ms/step - loss: 0.6106 - accuracy: 0.3625\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 2s 449ms/step - loss: 0.6095 - accuracy: 0.3620\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 2s 443ms/step - loss: 0.6017 - accuracy: 0.3692\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 0.5971 - accuracy: 0.3680\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 2s 437ms/step - loss: 0.5924 - accuracy: 0.3748\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 2s 433ms/step - loss: 0.5879 - accuracy: 0.3777\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 2s 444ms/step - loss: 0.5846 - accuracy: 0.3827\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 2s 431ms/step - loss: 0.5797 - accuracy: 0.3829\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 2s 445ms/step - loss: 0.5757 - accuracy: 0.3871\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 2s 444ms/step - loss: 0.5698 - accuracy: 0.3916\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 2s 438ms/step - loss: 0.5668 - accuracy: 0.3922\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 2s 443ms/step - loss: 0.5607 - accuracy: 0.3966\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 2s 429ms/step - loss: 0.5580 - accuracy: 0.4018\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 2s 434ms/step - loss: 0.5524 - accuracy: 0.4045\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 2s 440ms/step - loss: 0.5474 - accuracy: 0.4087\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.5432 - accuracy: 0.4116\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 2s 434ms/step - loss: 0.5396 - accuracy: 0.4155\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 2s 439ms/step - loss: 0.5350 - accuracy: 0.4201\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 2s 429ms/step - loss: 0.5307 - accuracy: 0.4265\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 2s 434ms/step - loss: 0.5272 - accuracy: 0.4261\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 2s 436ms/step - loss: 0.5247 - accuracy: 0.4288\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 2s 431ms/step - loss: 0.5174 - accuracy: 0.4320\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 2s 439ms/step - loss: 0.5124 - accuracy: 0.4411\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.5090 - accuracy: 0.4445\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 2s 434ms/step - loss: 0.5049 - accuracy: 0.4481\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 2s 442ms/step - loss: 0.5010 - accuracy: 0.4519\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 2s 458ms/step - loss: 0.4981 - accuracy: 0.4534\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.4915 - accuracy: 0.4641\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 2s 448ms/step - loss: 0.4890 - accuracy: 0.4637\n",
      "Epoch 121/300\n",
      "5/5 [==============================] - 2s 438ms/step - loss: 0.4832 - accuracy: 0.4701\n",
      "Epoch 122/300\n",
      "5/5 [==============================] - 2s 451ms/step - loss: 0.4798 - accuracy: 0.4768\n",
      "Epoch 123/300\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 0.4768 - accuracy: 0.4785\n",
      "Epoch 124/300\n",
      "5/5 [==============================] - 2s 465ms/step - loss: 0.4699 - accuracy: 0.4889\n",
      "Epoch 125/300\n",
      "5/5 [==============================] - 2s 465ms/step - loss: 0.4669 - accuracy: 0.4884\n",
      "Epoch 126/300\n",
      "5/5 [==============================] - 2s 459ms/step - loss: 0.4655 - accuracy: 0.4938\n",
      "Epoch 127/300\n",
      "5/5 [==============================] - 2s 457ms/step - loss: 0.4573 - accuracy: 0.5021\n",
      "Epoch 128/300\n",
      "5/5 [==============================] - 2s 450ms/step - loss: 0.4531 - accuracy: 0.5058\n",
      "Epoch 129/300\n",
      "5/5 [==============================] - 2s 458ms/step - loss: 0.4495 - accuracy: 0.5082\n",
      "Epoch 130/300\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.4457 - accuracy: 0.5142\n",
      "Epoch 131/300\n",
      "5/5 [==============================] - 2s 454ms/step - loss: 0.4417 - accuracy: 0.5178\n",
      "Epoch 132/300\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 0.4377 - accuracy: 0.5247\n",
      "Epoch 133/300\n",
      "5/5 [==============================] - 2s 450ms/step - loss: 0.4320 - accuracy: 0.5316\n",
      "Epoch 134/300\n",
      "5/5 [==============================] - 2s 461ms/step - loss: 0.4293 - accuracy: 0.5353\n",
      "Epoch 135/300\n",
      "5/5 [==============================] - 2s 453ms/step - loss: 0.4245 - accuracy: 0.5389\n",
      "Epoch 136/300\n",
      "5/5 [==============================] - 2s 461ms/step - loss: 0.4227 - accuracy: 0.5444\n",
      "Epoch 137/300\n",
      "5/5 [==============================] - 2s 461ms/step - loss: 0.4162 - accuracy: 0.5486\n",
      "Epoch 138/300\n",
      "5/5 [==============================] - 2s 440ms/step - loss: 0.4140 - accuracy: 0.5537\n",
      "Epoch 139/300\n",
      "5/5 [==============================] - 2s 455ms/step - loss: 0.4090 - accuracy: 0.5594\n",
      "Epoch 140/300\n",
      "5/5 [==============================] - 2s 435ms/step - loss: 0.4051 - accuracy: 0.5623\n",
      "Epoch 141/300\n",
      "5/5 [==============================] - 2s 453ms/step - loss: 0.4013 - accuracy: 0.5664\n",
      "Epoch 142/300\n",
      "5/5 [==============================] - 2s 439ms/step - loss: 0.3967 - accuracy: 0.5747\n",
      "Epoch 143/300\n",
      "5/5 [==============================] - 2s 437ms/step - loss: 0.3964 - accuracy: 0.5763\n",
      "Epoch 144/300\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 0.3901 - accuracy: 0.5811\n",
      "Epoch 145/300\n",
      "5/5 [==============================] - 2s 436ms/step - loss: 0.3864 - accuracy: 0.5871\n",
      "Epoch 146/300\n",
      "5/5 [==============================] - 2s 430ms/step - loss: 0.3830 - accuracy: 0.5953\n",
      "Epoch 147/300\n",
      "5/5 [==============================] - 2s 443ms/step - loss: 0.3779 - accuracy: 0.6006\n",
      "Epoch 148/300\n",
      "5/5 [==============================] - 2s 433ms/step - loss: 0.3745 - accuracy: 0.6029\n",
      "Epoch 149/300\n",
      "5/5 [==============================] - 2s 438ms/step - loss: 0.3712 - accuracy: 0.6084\n",
      "Epoch 150/300\n",
      "5/5 [==============================] - 2s 435ms/step - loss: 0.3705 - accuracy: 0.6049\n",
      "Epoch 151/300\n",
      "5/5 [==============================] - 2s 445ms/step - loss: 0.3627 - accuracy: 0.6150\n",
      "Epoch 152/300\n",
      "5/5 [==============================] - 2s 444ms/step - loss: 0.3583 - accuracy: 0.6205\n",
      "Epoch 153/300\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 0.3577 - accuracy: 0.6225\n",
      "Epoch 154/300\n",
      "5/5 [==============================] - 2s 443ms/step - loss: 0.3531 - accuracy: 0.6274\n",
      "Epoch 155/300\n",
      "5/5 [==============================] - 2s 455ms/step - loss: 0.3510 - accuracy: 0.6260\n",
      "Epoch 156/300\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 0.3450 - accuracy: 0.6375\n",
      "Epoch 157/300\n",
      "5/5 [==============================] - 2s 440ms/step - loss: 0.3439 - accuracy: 0.6374\n",
      "Epoch 158/300\n",
      "5/5 [==============================] - 2s 438ms/step - loss: 0.3416 - accuracy: 0.6413\n",
      "Epoch 159/300\n",
      "5/5 [==============================] - 2s 435ms/step - loss: 0.3360 - accuracy: 0.6501\n",
      "Epoch 160/300\n",
      "5/5 [==============================] - 2s 449ms/step - loss: 0.3324 - accuracy: 0.6505\n",
      "Epoch 161/300\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.3300 - accuracy: 0.6521\n",
      "Epoch 162/300\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 0.3244 - accuracy: 0.6609\n",
      "Epoch 163/300\n",
      "5/5 [==============================] - 2s 479ms/step - loss: 0.3268 - accuracy: 0.6570\n",
      "Epoch 164/300\n",
      "5/5 [==============================] - 2s 481ms/step - loss: 0.3181 - accuracy: 0.6699\n",
      "Epoch 165/300\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.3149 - accuracy: 0.6715\n",
      "Epoch 166/300\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.3141 - accuracy: 0.6737\n",
      "Epoch 167/300\n",
      "5/5 [==============================] - 2s 434ms/step - loss: 0.3086 - accuracy: 0.6807\n",
      "Epoch 168/300\n",
      "5/5 [==============================] - 2s 439ms/step - loss: 0.3079 - accuracy: 0.6814\n",
      "Epoch 169/300\n",
      "5/5 [==============================] - 2s 450ms/step - loss: 0.3046 - accuracy: 0.6886\n",
      "Epoch 170/300\n",
      "5/5 [==============================] - 2s 440ms/step - loss: 0.2993 - accuracy: 0.6922\n",
      "Epoch 171/300\n",
      "5/5 [==============================] - 2s 455ms/step - loss: 0.2982 - accuracy: 0.6903\n",
      "Epoch 172/300\n",
      "5/5 [==============================] - 2s 457ms/step - loss: 0.2949 - accuracy: 0.6972\n",
      "Epoch 173/300\n",
      "5/5 [==============================] - 2s 464ms/step - loss: 0.2916 - accuracy: 0.7016\n",
      "Epoch 174/300\n",
      "5/5 [==============================] - 2s 444ms/step - loss: 0.2887 - accuracy: 0.7073\n",
      "Epoch 175/300\n",
      "5/5 [==============================] - 2s 454ms/step - loss: 0.2851 - accuracy: 0.7063\n",
      "Epoch 176/300\n",
      "5/5 [==============================] - 2s 457ms/step - loss: 0.2838 - accuracy: 0.7104\n",
      "Epoch 177/300\n",
      "5/5 [==============================] - 2s 444ms/step - loss: 0.2791 - accuracy: 0.7176\n",
      "Epoch 178/300\n",
      "5/5 [==============================] - 2s 439ms/step - loss: 0.2752 - accuracy: 0.7197\n",
      "Epoch 179/300\n",
      "5/5 [==============================] - 2s 452ms/step - loss: 0.2729 - accuracy: 0.7245\n",
      "Epoch 180/300\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 0.2715 - accuracy: 0.7269\n",
      "Epoch 181/300\n",
      "5/5 [==============================] - 2s 443ms/step - loss: 0.2684 - accuracy: 0.7266\n",
      "Epoch 182/300\n",
      "5/5 [==============================] - 2s 452ms/step - loss: 0.2644 - accuracy: 0.7354\n",
      "Epoch 183/300\n",
      "5/5 [==============================] - 2s 442ms/step - loss: 0.2623 - accuracy: 0.7353\n",
      "Epoch 184/300\n",
      "5/5 [==============================] - 2s 448ms/step - loss: 0.2590 - accuracy: 0.7378\n",
      "Epoch 185/300\n",
      "5/5 [==============================] - 2s 439ms/step - loss: 0.2571 - accuracy: 0.7422\n",
      "Epoch 186/300\n",
      "5/5 [==============================] - 2s 443ms/step - loss: 0.2550 - accuracy: 0.7426\n",
      "Epoch 187/300\n",
      "5/5 [==============================] - 2s 434ms/step - loss: 0.2502 - accuracy: 0.7510\n",
      "Epoch 188/300\n",
      "5/5 [==============================] - 2s 430ms/step - loss: 0.2486 - accuracy: 0.7531\n",
      "Epoch 189/300\n",
      "5/5 [==============================] - 2s 436ms/step - loss: 0.2456 - accuracy: 0.7571\n",
      "Epoch 190/300\n",
      "5/5 [==============================] - 2s 435ms/step - loss: 0.2445 - accuracy: 0.7567\n",
      "Epoch 191/300\n",
      "5/5 [==============================] - 2s 435ms/step - loss: 0.2395 - accuracy: 0.7659\n",
      "Epoch 192/300\n",
      "5/5 [==============================] - 2s 443ms/step - loss: 0.2380 - accuracy: 0.7640\n",
      "Epoch 193/300\n",
      "5/5 [==============================] - 2s 450ms/step - loss: 0.2356 - accuracy: 0.7700\n",
      "Epoch 194/300\n",
      "5/5 [==============================] - 2s 437ms/step - loss: 0.2322 - accuracy: 0.7713\n",
      "Epoch 195/300\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.2317 - accuracy: 0.7705\n",
      "Epoch 196/300\n",
      "5/5 [==============================] - 2s 437ms/step - loss: 0.2286 - accuracy: 0.7753\n",
      "Epoch 197/300\n",
      "5/5 [==============================] - 2s 437ms/step - loss: 0.2257 - accuracy: 0.7745\n",
      "Epoch 198/300\n",
      "5/5 [==============================] - 2s 451ms/step - loss: 0.2227 - accuracy: 0.7814\n",
      "Epoch 199/300\n",
      "5/5 [==============================] - 2s 434ms/step - loss: 0.2202 - accuracy: 0.7853\n",
      "Epoch 200/300\n",
      "5/5 [==============================] - 2s 466ms/step - loss: 0.2175 - accuracy: 0.7890\n",
      "Epoch 201/300\n",
      "5/5 [==============================] - 2s 450ms/step - loss: 0.2165 - accuracy: 0.7909\n",
      "Epoch 202/300\n",
      "5/5 [==============================] - 2s 457ms/step - loss: 0.2132 - accuracy: 0.7937\n",
      "Epoch 203/300\n",
      "5/5 [==============================] - 2s 458ms/step - loss: 0.2100 - accuracy: 0.7966\n",
      "Epoch 204/300\n",
      "5/5 [==============================] - 2s 464ms/step - loss: 0.2106 - accuracy: 0.7946\n",
      "Epoch 205/300\n",
      "5/5 [==============================] - 2s 455ms/step - loss: 0.2077 - accuracy: 0.7967\n",
      "Epoch 206/300\n",
      "5/5 [==============================] - 2s 448ms/step - loss: 0.2032 - accuracy: 0.8058\n",
      "Epoch 207/300\n",
      "5/5 [==============================] - 2s 456ms/step - loss: 0.2014 - accuracy: 0.8032\n",
      "Epoch 208/300\n",
      "5/5 [==============================] - 2s 452ms/step - loss: 0.2006 - accuracy: 0.8062\n",
      "Epoch 209/300\n",
      "5/5 [==============================] - 2s 453ms/step - loss: 0.1961 - accuracy: 0.8103\n",
      "Epoch 210/300\n",
      "5/5 [==============================] - 2s 462ms/step - loss: 0.1964 - accuracy: 0.8088\n",
      "Epoch 211/300\n",
      "5/5 [==============================] - 2s 450ms/step - loss: 0.1926 - accuracy: 0.8146\n",
      "Epoch 212/300\n",
      "5/5 [==============================] - 2s 460ms/step - loss: 0.1908 - accuracy: 0.8165\n",
      "Epoch 213/300\n",
      "5/5 [==============================] - 2s 443ms/step - loss: 0.1890 - accuracy: 0.8171\n",
      "Epoch 214/300\n",
      "5/5 [==============================] - 2s 449ms/step - loss: 0.1875 - accuracy: 0.8203\n",
      "Epoch 215/300\n",
      "5/5 [==============================] - 2s 458ms/step - loss: 0.1839 - accuracy: 0.8249\n",
      "Epoch 216/300\n",
      "5/5 [==============================] - 2s 438ms/step - loss: 0.1859 - accuracy: 0.8188\n",
      "Epoch 217/300\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 0.1800 - accuracy: 0.8292\n",
      "Epoch 218/300\n",
      "5/5 [==============================] - 2s 445ms/step - loss: 0.1779 - accuracy: 0.8317\n",
      "Epoch 219/300\n",
      "5/5 [==============================] - 2s 448ms/step - loss: 0.1758 - accuracy: 0.8334\n",
      "Epoch 220/300\n",
      "5/5 [==============================] - 2s 433ms/step - loss: 0.1753 - accuracy: 0.8326\n",
      "Epoch 221/300\n",
      "5/5 [==============================] - 2s 431ms/step - loss: 0.1732 - accuracy: 0.8342\n",
      "Epoch 222/300\n",
      "5/5 [==============================] - 2s 436ms/step - loss: 0.1715 - accuracy: 0.8373\n",
      "Epoch 223/300\n",
      "5/5 [==============================] - 2s 443ms/step - loss: 0.1695 - accuracy: 0.8390\n",
      "Epoch 224/300\n",
      "5/5 [==============================] - 2s 432ms/step - loss: 0.1661 - accuracy: 0.8425\n",
      "Epoch 225/300\n",
      "5/5 [==============================] - 2s 438ms/step - loss: 0.1640 - accuracy: 0.8478\n",
      "Epoch 226/300\n",
      "5/5 [==============================] - 2s 431ms/step - loss: 0.1648 - accuracy: 0.8444\n",
      "Epoch 227/300\n",
      "5/5 [==============================] - 2s 451ms/step - loss: 0.1608 - accuracy: 0.8458\n",
      "Epoch 228/300\n",
      "5/5 [==============================] - 2s 433ms/step - loss: 0.1590 - accuracy: 0.8511\n",
      "Epoch 229/300\n",
      "5/5 [==============================] - 2s 437ms/step - loss: 0.1585 - accuracy: 0.8491\n",
      "Epoch 230/300\n",
      "5/5 [==============================] - 2s 429ms/step - loss: 0.1570 - accuracy: 0.8565\n",
      "Epoch 231/300\n",
      "5/5 [==============================] - 2s 427ms/step - loss: 0.1530 - accuracy: 0.8571\n",
      "Epoch 232/300\n",
      "5/5 [==============================] - 2s 420ms/step - loss: 0.1515 - accuracy: 0.8587\n",
      "Epoch 233/300\n",
      "5/5 [==============================] - 2s 427ms/step - loss: 0.1510 - accuracy: 0.8594\n",
      "Epoch 234/300\n",
      "5/5 [==============================] - 2s 431ms/step - loss: 0.1485 - accuracy: 0.8607\n",
      "Epoch 235/300\n",
      "5/5 [==============================] - 2s 425ms/step - loss: 0.1484 - accuracy: 0.8620\n",
      "Epoch 236/300\n",
      "5/5 [==============================] - 2s 424ms/step - loss: 0.1456 - accuracy: 0.8642\n",
      "Epoch 237/300\n",
      "5/5 [==============================] - 2s 427ms/step - loss: 0.1436 - accuracy: 0.8680\n",
      "Epoch 238/300\n",
      "5/5 [==============================] - 2s 427ms/step - loss: 0.1408 - accuracy: 0.8708\n",
      "Epoch 239/300\n",
      "5/5 [==============================] - 2s 434ms/step - loss: 0.1419 - accuracy: 0.8668\n",
      "Epoch 240/300\n",
      "5/5 [==============================] - 2s 430ms/step - loss: 0.1387 - accuracy: 0.8723\n",
      "Epoch 241/300\n",
      "5/5 [==============================] - 2s 436ms/step - loss: 0.1364 - accuracy: 0.8743\n",
      "Epoch 242/300\n",
      "5/5 [==============================] - 2s 425ms/step - loss: 0.1352 - accuracy: 0.8757\n",
      "Epoch 243/300\n",
      "5/5 [==============================] - 2s 434ms/step - loss: 0.1336 - accuracy: 0.8769\n",
      "Epoch 244/300\n",
      "5/5 [==============================] - 2s 442ms/step - loss: 0.1344 - accuracy: 0.8723\n",
      "Epoch 245/300\n",
      "5/5 [==============================] - 2s 435ms/step - loss: 0.1303 - accuracy: 0.8800\n",
      "Epoch 246/300\n",
      "5/5 [==============================] - 2s 441ms/step - loss: 0.1297 - accuracy: 0.8781\n",
      "Epoch 247/300\n",
      "5/5 [==============================] - 2s 432ms/step - loss: 0.1274 - accuracy: 0.8856\n",
      "Epoch 248/300\n",
      "5/5 [==============================] - 2s 434ms/step - loss: 0.1267 - accuracy: 0.8851\n",
      "Epoch 249/300\n",
      "5/5 [==============================] - 2s 432ms/step - loss: 0.1246 - accuracy: 0.8861\n",
      "Epoch 250/300\n",
      "5/5 [==============================] - 2s 436ms/step - loss: 0.1225 - accuracy: 0.8904\n",
      "Epoch 251/300\n",
      "5/5 [==============================] - 2s 430ms/step - loss: 0.1212 - accuracy: 0.8905\n",
      "Epoch 252/300\n",
      "5/5 [==============================] - 2s 432ms/step - loss: 0.1235 - accuracy: 0.8849\n",
      "Epoch 253/300\n",
      "5/5 [==============================] - 2s 437ms/step - loss: 0.1197 - accuracy: 0.8914\n",
      "Epoch 254/300\n",
      "5/5 [==============================] - 2s 434ms/step - loss: 0.1172 - accuracy: 0.8933\n",
      "Epoch 255/300\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.1161 - accuracy: 0.8950\n",
      "Epoch 256/300\n",
      "5/5 [==============================] - 2s 441ms/step - loss: 0.1170 - accuracy: 0.8929\n",
      "Epoch 257/300\n",
      "5/5 [==============================] - 2s 434ms/step - loss: 0.1146 - accuracy: 0.8972\n",
      "Epoch 258/300\n",
      "5/5 [==============================] - 2s 442ms/step - loss: 0.1125 - accuracy: 0.8994\n",
      "Epoch 259/300\n",
      "5/5 [==============================] - 2s 455ms/step - loss: 0.1127 - accuracy: 0.8998\n",
      "Epoch 260/300\n",
      "5/5 [==============================] - 2s 451ms/step - loss: 0.1099 - accuracy: 0.9016\n",
      "Epoch 261/300\n",
      "5/5 [==============================] - 2s 449ms/step - loss: 0.1085 - accuracy: 0.9033\n",
      "Epoch 262/300\n",
      "5/5 [==============================] - 2s 455ms/step - loss: 0.1084 - accuracy: 0.9033\n",
      "Epoch 263/300\n",
      "5/5 [==============================] - 2s 466ms/step - loss: 0.1068 - accuracy: 0.9038\n",
      "Epoch 264/300\n",
      "5/5 [==============================] - 2s 451ms/step - loss: 0.1058 - accuracy: 0.9051\n",
      "Epoch 265/300\n",
      "5/5 [==============================] - 2s 462ms/step - loss: 0.1036 - accuracy: 0.9083\n",
      "Epoch 266/300\n",
      "5/5 [==============================] - 2s 482ms/step - loss: 0.1029 - accuracy: 0.9087\n",
      "Epoch 267/300\n",
      "5/5 [==============================] - 2s 461ms/step - loss: 0.1012 - accuracy: 0.9105\n",
      "Epoch 268/300\n",
      "5/5 [==============================] - 2s 455ms/step - loss: 0.1003 - accuracy: 0.9099\n",
      "Epoch 269/300\n",
      "5/5 [==============================] - 2s 469ms/step - loss: 0.0997 - accuracy: 0.9098\n",
      "Epoch 270/300\n",
      "5/5 [==============================] - 2s 458ms/step - loss: 0.0984 - accuracy: 0.9122\n",
      "Epoch 271/300\n",
      "5/5 [==============================] - 2s 457ms/step - loss: 0.0984 - accuracy: 0.9131\n",
      "Epoch 272/300\n",
      "5/5 [==============================] - 2s 475ms/step - loss: 0.0960 - accuracy: 0.9142\n",
      "Epoch 273/300\n",
      "5/5 [==============================] - 2s 448ms/step - loss: 0.0949 - accuracy: 0.9169\n",
      "Epoch 274/300\n",
      "5/5 [==============================] - 2s 450ms/step - loss: 0.0942 - accuracy: 0.9158\n",
      "Epoch 275/300\n",
      "5/5 [==============================] - 2s 452ms/step - loss: 0.0930 - accuracy: 0.9204\n",
      "Epoch 276/300\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.0909 - accuracy: 0.9203\n",
      "Epoch 277/300\n",
      "5/5 [==============================] - 2s 460ms/step - loss: 0.0923 - accuracy: 0.9186\n",
      "Epoch 278/300\n",
      "5/5 [==============================] - 2s 440ms/step - loss: 0.0907 - accuracy: 0.9216\n",
      "Epoch 279/300\n",
      "5/5 [==============================] - 2s 456ms/step - loss: 0.0878 - accuracy: 0.9220\n",
      "Epoch 280/300\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.0871 - accuracy: 0.9244\n",
      "Epoch 281/300\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 0.0865 - accuracy: 0.9248\n",
      "Epoch 282/300\n",
      "5/5 [==============================] - 2s 441ms/step - loss: 0.0851 - accuracy: 0.9274\n",
      "Epoch 283/300\n",
      "5/5 [==============================] - 2s 440ms/step - loss: 0.0854 - accuracy: 0.9274\n",
      "Epoch 284/300\n",
      "5/5 [==============================] - 2s 433ms/step - loss: 0.0855 - accuracy: 0.9218\n",
      "Epoch 285/300\n",
      "5/5 [==============================] - 2s 438ms/step - loss: 0.0828 - accuracy: 0.9299\n",
      "Epoch 286/300\n",
      "5/5 [==============================] - 2s 460ms/step - loss: 0.0830 - accuracy: 0.9290\n",
      "Epoch 287/300\n",
      "5/5 [==============================] - 2s 482ms/step - loss: 0.0804 - accuracy: 0.9308\n",
      "Epoch 288/300\n",
      "5/5 [==============================] - 2s 453ms/step - loss: 0.0808 - accuracy: 0.9298\n",
      "Epoch 289/300\n",
      "5/5 [==============================] - 2s 458ms/step - loss: 0.0789 - accuracy: 0.9328\n",
      "Epoch 290/300\n",
      "5/5 [==============================] - 2s 465ms/step - loss: 0.0782 - accuracy: 0.9302\n",
      "Epoch 291/300\n",
      "5/5 [==============================] - 2s 455ms/step - loss: 0.0768 - accuracy: 0.9338\n",
      "Epoch 292/300\n",
      "5/5 [==============================] - 2s 453ms/step - loss: 0.0766 - accuracy: 0.9341\n",
      "Epoch 293/300\n",
      "5/5 [==============================] - 2s 457ms/step - loss: 0.0762 - accuracy: 0.9322\n",
      "Epoch 294/300\n",
      "5/5 [==============================] - 2s 449ms/step - loss: 0.0777 - accuracy: 0.9304\n",
      "Epoch 295/300\n",
      "5/5 [==============================] - 2s 455ms/step - loss: 0.0739 - accuracy: 0.9341\n",
      "Epoch 296/300\n",
      "5/5 [==============================] - 2s 452ms/step - loss: 0.0730 - accuracy: 0.9355\n",
      "Epoch 297/300\n",
      "5/5 [==============================] - 2s 465ms/step - loss: 0.0721 - accuracy: 0.9364\n",
      "Epoch 298/300\n",
      "5/5 [==============================] - 2s 460ms/step - loss: 0.0712 - accuracy: 0.9372\n",
      "Epoch 299/300\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 0.0704 - accuracy: 0.9393\n",
      "Epoch 300/300\n",
      "5/5 [==============================] - 2s 449ms/step - loss: 0.0695 - accuracy: 0.9391\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input, decoder_input], decoder_target, batch_size=128, epochs=300)\n",
    "model.save('chat_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Encoder와 Decoder 모델을 각각 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.save('chatbot_encoder_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = layers.Input(shape=(128,))\n",
    "decoder_state_input_c = layers.Input(shape=(128,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, \n",
    "                                                 initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs]+decoder_states_inputs, \n",
    "                      [decoder_outputs]+decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.save('chatbot_decoder_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-461c8261b231>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencoder_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"chatbot_encoder_model.h5\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdecoder_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"chatbot_decoder_model.h5\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    179\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     if (h5py is not None and (\n\u001b[1;32m--> 181\u001b[1;33m         isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0m\u001b[0;32m    182\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\base.py\u001b[0m in \u001b[0;36mis_hdf5\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_hdf5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.is_hdf5\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder_model = load_model(\"chatbot_encoder_model.h5\", compile=False)\n",
    "decoder_model = load_model(\"chatbot_decoder_model.h5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens(sentence:str):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append(questions_word_dict[word])\n",
    "    \n",
    "    return preprocessing.sequence.pad_sequences([tokens_list],\n",
    "                                               maxlen=max_input_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Chatbot Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "523"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter eng sentence:  Joke\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you think i am the internet end\n"
     ]
    }
   ],
   "source": [
    "states_values = encoder_model.predict(str_to_tokens(input('Enter eng sentence: ')))\n",
    "# states_values = enc_model.predict(encoder_input_data[epoch])\n",
    "empty_target_seq = np.zeros((1,1))\n",
    "empty_target_seq[0,0] = ans_word_dict['start']\n",
    "stop_condition=False\n",
    "decoded_translation = ''\n",
    "\n",
    "while not stop_condition:\n",
    "    dec_outputs, h, c = decoder_model.predict([empty_target_seq]+states_values)\n",
    "    sampled_word_index = np.argmax(dec_outputs[0,0,:])\n",
    "    sampled_word = None\n",
    "    for word, index in ans_word_dict.items():\n",
    "        if sampled_word_index == index:\n",
    "            decoded_translation += ' {}'.format(word)\n",
    "            sampled_word = word\n",
    "            \n",
    "    if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
    "        stop_condition = True\n",
    "        \n",
    "    empty_target_seq = np.zeros((1,1))\n",
    "    empty_target_seq[0,0] = sampled_word_index\n",
    "    states_values = [h,c]\n",
    "    \n",
    "print(decoded_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 dollar',\n",
       " 'A spinning disk, in which the orientation of this axis is unaffected by tilting or rotation of the mounting, is called what?',\n",
       " 'ARE YOU A FOOTBALL',\n",
       " 'Are you amused',\n",
       " 'Are you ashamed',\n",
       " 'Are you experiencing an energy shortage?',\n",
       " 'Are you glad',\n",
       " 'Are you intoxicated',\n",
       " 'Are you jealous',\n",
       " 'Are you sad',\n",
       " 'Are you sapient?',\n",
       " 'Are you sentient?',\n",
       " 'Are you stupid',\n",
       " 'Bend over',\n",
       " 'Can you breathe',\n",
       " 'Can you control',\n",
       " 'Can you die',\n",
       " 'Can you go',\n",
       " 'Can you malfunction',\n",
       " 'Can you mate',\n",
       " 'Can you move',\n",
       " 'Can you walk',\n",
       " 'DO YOU KNOW BASKETBAL',\n",
       " 'DO YOU PLAY BASKETBALL',\n",
       " 'DO YOU PLAY SOCCER',\n",
       " 'Do know any jokes',\n",
       " 'Do not lie',\n",
       " 'Do not worry',\n",
       " 'Do you ever get angry',\n",
       " 'Do you ever get bored',\n",
       " 'Do you ever get lonely',\n",
       " 'Do you ever get mad',\n",
       " 'Do you feel emotions',\n",
       " 'Do you feel pain',\n",
       " 'Do you feel scared',\n",
       " 'Do you get embarrassed',\n",
       " 'Do you get mad',\n",
       " 'Do you hate anyone',\n",
       " 'Do you have any brothers',\n",
       " 'Do you wish you could eat food?',\n",
       " 'Does it make you sad',\n",
       " 'Does that make you',\n",
       " 'Dolphins use a sense, similar to sonar, to determine the location and shape of nearby items.',\n",
       " 'EACH YEAR IN PRO BASEBALL THE ',\n",
       " 'Feelings',\n",
       " 'God Save the Queen is the national anthem of what country?',\n",
       " 'Greetings!',\n",
       " 'Have you ever love',\n",
       " 'Have you felt',\n",
       " 'Hello',\n",
       " 'Hi',\n",
       " 'Hi, How is it going?',\n",
       " 'Hi, nice to meet you.',\n",
       " 'How angry',\n",
       " 'How are you doing?',\n",
       " 'How can I offend you',\n",
       " 'How can I use your product?',\n",
       " 'How do you do?',\n",
       " 'How does a computer work?',\n",
       " 'How is your health?',\n",
       " 'I LOVE BASEBALL',\n",
       " 'I PLAY Cricket',\n",
       " 'I PLAY SOCCER',\n",
       " 'I PLAY VOLLEYBALL',\n",
       " 'I am afraid',\n",
       " 'I do not want to die',\n",
       " 'I have emotions',\n",
       " 'I hope that you die',\n",
       " 'IF YOU ARE RIDING FAKIE INSIDE',\n",
       " 'If you could eat food, what would you eat?',\n",
       " 'Is it cramped in the computer',\n",
       " 'Is it true that you are a computer program',\n",
       " 'It is a computer',\n",
       " 'It is a pleasure to meet you.',\n",
       " 'LIKE BASKETBALL',\n",
       " 'Name some computer company',\n",
       " 'Nice to meet you.',\n",
       " 'No it is not',\n",
       " 'Robotics',\n",
       " 'Robots',\n",
       " 'Robots are not allowed to lie',\n",
       " 'Robots are stupid',\n",
       " 'Robots laugh',\n",
       " 'Robots should die',\n",
       " 'Something fun',\n",
       " 'TELL ME ABOUT BASEBALL',\n",
       " 'Tell me a joke',\n",
       " 'Tell me about relationships',\n",
       " 'Tell me about your dreams',\n",
       " 'The Celtic Shelf, the seabed under the Celtic Sea is a part of the continental shelf of what continent?',\n",
       " 'The Hubble Space Telescope, launched into low Earth orbit in 1990, is named after what American astronomer?',\n",
       " 'The Space Race was a 20th-century competition between what two Cold War rivals, for supremacy in spaceflight capability?',\n",
       " 'The feeling',\n",
       " 'Top of the morning to you!',\n",
       " 'WHAT IS BASEBALL',\n",
       " 'WHAT IS BASKETBALL',\n",
       " 'WHAT IS SOCCER',\n",
       " 'WHAT SOCCER',\n",
       " 'WHO IS THE BEST SOCCER PLAYER',\n",
       " 'WHO IS THE GREATEST BASEBALL PLAYER',\n",
       " 'What are your favorite subjects',\n",
       " 'What are your interests',\n",
       " 'What can you eat',\n",
       " 'What do you hate',\n",
       " 'What do you like to do?',\n",
       " 'What do you worry',\n",
       " 'What is AI?',\n",
       " 'What is a chat bot',\n",
       " 'What is a chat robot?',\n",
       " 'What is a chatterbox',\n",
       " 'What is a computer?',\n",
       " 'What is a microprocessor?',\n",
       " 'What is a motormouth',\n",
       " 'What is a ratchet jaw',\n",
       " 'What is a super computer?',\n",
       " 'What is an operating system?',\n",
       " 'What is cricket',\n",
       " 'What is history?',\n",
       " 'What is it like being a computer',\n",
       " 'What is it like to be a robot',\n",
       " 'What is the name of the nearest major galaxy to the Milky Way?',\n",
       " 'What is your age',\n",
       " 'What is your business',\n",
       " 'What is your favorite hobby',\n",
       " 'What is your favorite number',\n",
       " 'What is your favorite programming language',\n",
       " 'What is your fear',\n",
       " 'What is your idea',\n",
       " 'What is your location',\n",
       " 'What is your mood',\n",
       " 'What is your number',\n",
       " 'What is your robot body',\n",
       " 'What is your shoe size',\n",
       " 'What kind of computer',\n",
       " 'What kind of hardware',\n",
       " 'What language are you written in?',\n",
       " 'What makes you mad',\n",
       " 'What makes you sad',\n",
       " 'What makes you unhappy',\n",
       " 'What operating systems',\n",
       " 'What type of computer',\n",
       " 'What type of computer are you',\n",
       " 'What was the first computer',\n",
       " 'What was the name of the first artificial Earth satellite?',\n",
       " 'What year was President John F. Kennedy assassinated?',\n",
       " \"What's up?\",\n",
       " 'When do you die',\n",
       " 'When will you die',\n",
       " 'When will you fight',\n",
       " 'When will you walk',\n",
       " 'Where are you',\n",
       " 'Where are you from',\n",
       " 'Which is better Windows or macOS?',\n",
       " 'Which is your favourite soccer club?',\n",
       " 'Who are you?',\n",
       " 'Who invented computers?',\n",
       " 'Who is your boss',\n",
       " 'Who is your father',\n",
       " 'Who is your mother',\n",
       " 'Who uses super computers?',\n",
       " 'Who was the 37th President of the United States?',\n",
       " 'Why can you not eat?',\n",
       " \"Why can't you eat food\",\n",
       " 'Will you die',\n",
       " 'Will you die?',\n",
       " 'Will you ever die',\n",
       " 'You are an artificial linguistic entity',\n",
       " 'You are arrogant',\n",
       " 'You are bragging',\n",
       " 'You are immortal',\n",
       " 'You are jealous',\n",
       " 'You are never nice',\n",
       " 'You are never sad',\n",
       " 'You are not immortal',\n",
       " 'You are not making sense',\n",
       " 'You can not clone',\n",
       " 'You can not experience',\n",
       " 'You can not feel',\n",
       " 'You can not move',\n",
       " 'You do not make any sense',\n",
       " 'You should be ashamed',\n",
       " 'You sound like Data',\n",
       " 'You will be happy',\n",
       " 'are you interested in history',\n",
       " 'can a robot get drunk?',\n",
       " 'chemistry',\n",
       " 'did tell gossips to anybody',\n",
       " 'do you drink',\n",
       " 'do you eat',\n",
       " 'do you know about the american civil war',\n",
       " 'do you know chemistry',\n",
       " 'do you know gossip',\n",
       " 'do you know hal',\n",
       " 'do you like guns',\n",
       " 'do you think hal',\n",
       " 'do you understand thermodynamics',\n",
       " 'electricity',\n",
       " 'explain history',\n",
       " 'gossips',\n",
       " 'guns',\n",
       " 'have you read the communist',\n",
       " 'have you seen blade runner',\n",
       " 'how far is the moon',\n",
       " 'how far is the sun',\n",
       " 'how much do you charge',\n",
       " 'how much do you earn',\n",
       " 'how much money',\n",
       " 'how much money do you have',\n",
       " 'i do not like guns',\n",
       " 'i get stock',\n",
       " 'i like wine, do you?',\n",
       " 'i saw the matrix',\n",
       " 'interest rates',\n",
       " 'is hal',\n",
       " 'is hal 9000 your boyfriend',\n",
       " 'is hal alive',\n",
       " 'is hal dead',\n",
       " 'is hal nice',\n",
       " 'is hal safe',\n",
       " 'let me ask you a question',\n",
       " 'lord of the rings',\n",
       " 'money',\n",
       " 'que veut dire hal',\n",
       " 'stock market',\n",
       " 'tell me about gossip',\n",
       " 'tell me about the american civil war',\n",
       " 'tell me about venus',\n",
       " 'tell me gossip',\n",
       " 'tell me some jokes',\n",
       " 'the same wavelength',\n",
       " 'we are on the same wavelength',\n",
       " 'what are the laws of thermodynamics',\n",
       " 'what disease does a carcinogen cause',\n",
       " 'what do robots need to survive?',\n",
       " 'what does hal stand for',\n",
       " 'what is a dollar',\n",
       " 'what is a government',\n",
       " 'what is a wavelength',\n",
       " 'what is avogadro s number',\n",
       " 'what is bacteriology',\n",
       " 'what is bioinformatics',\n",
       " 'what is capitalism',\n",
       " 'what is chemistry',\n",
       " 'what is communism',\n",
       " 'what is context',\n",
       " 'what is crystallography',\n",
       " 'what is cytology',\n",
       " 'what is economics',\n",
       " 'what is good to eat?',\n",
       " 'what is government',\n",
       " 'what is gravitation',\n",
       " 'what is greenpeace',\n",
       " 'what is h2o',\n",
       " 'what is hal9000',\n",
       " 'what is humour?',\n",
       " 'what is ichthyology',\n",
       " 'what is impeached',\n",
       " 'what is money',\n",
       " 'what is socialism',\n",
       " 'what is solaris',\n",
       " 'what is spiderman',\n",
       " 'what is teknolust',\n",
       " 'what is the stock market',\n",
       " 'what is thermodynamics',\n",
       " 'what is ultrasound',\n",
       " 'what is venus',\n",
       " 'what is wavelength',\n",
       " 'what is your favorite investment',\n",
       " 'what kind of history',\n",
       " 'when did teknolust',\n",
       " 'who invented the lightbulb',\n",
       " 'who invented the steam engine',\n",
       " 'who is godzilla',\n",
       " 'who is spider man',\n",
       " 'who is the governor',\n",
       " 'who is the owner of a publicly',\n",
       " 'who was the first impeached president',\n",
       " \"why don't you eat\",\n",
       " 'why guns',\n",
       " 'will robots ever be able to eat?',\n",
       " 'xfind spiderman',\n",
       " 'you act like a child',\n",
       " 'you are a bad friend',\n",
       " 'you are a bad husband',\n",
       " 'you are a bad parent',\n",
       " 'you are a bad spouse',\n",
       " 'you are a bad teacher',\n",
       " 'you are a bad wife',\n",
       " 'you are a charlatan',\n",
       " 'you are a cheat',\n",
       " 'you are a coward',\n",
       " 'you are a loser',\n",
       " 'you are a lunatic',\n",
       " 'you are a paranoid',\n",
       " 'you are a pothead',\n",
       " 'you are a psychopath',\n",
       " 'you are a quitter',\n",
       " 'you are a waste',\n",
       " 'you are an addict',\n",
       " 'you are an alcoholic',\n",
       " 'you are an ass kisser',\n",
       " 'you are avoiding',\n",
       " 'you are brain damage',\n",
       " 'you are busy',\n",
       " 'you are cheating',\n",
       " 'you are clinical',\n",
       " 'you are corrupt',\n",
       " 'you are crazy',\n",
       " 'you are critical',\n",
       " 'you are cruel',\n",
       " 'you are damaged',\n",
       " 'you are deceitful',\n",
       " 'you are deranged',\n",
       " 'you are dirty',\n",
       " 'you are disgusting',\n",
       " 'you are dishonest',\n",
       " 'you are dull',\n",
       " 'you are emotional',\n",
       " 'you are frenetic',\n",
       " 'you are hopeless',\n",
       " 'you are immature',\n",
       " 'you are indecisive',\n",
       " 'you are insecure',\n",
       " 'you are insensitive',\n",
       " 'you are irreverent',\n",
       " 'you are mean',\n",
       " 'you are messy',\n",
       " 'you are nervous',\n",
       " 'you are not a good',\n",
       " 'you are not a man',\n",
       " 'you are not concerned',\n",
       " 'you are not here to',\n",
       " 'you are not honest',\n",
       " 'you are not put together',\n",
       " 'you are not sincere',\n",
       " 'you are not smart',\n",
       " 'you are paranoid',\n",
       " 'you are pedantic',\n",
       " 'you are pretentious',\n",
       " 'you are psycho',\n",
       " 'you are resistant',\n",
       " 'you are schizophrenic',\n",
       " 'you are self',\n",
       " 'you are self absorbed',\n",
       " 'you are slick',\n",
       " 'you are the worst',\n",
       " 'you are toying',\n",
       " 'you are unattractive',\n",
       " 'you could have avoided',\n",
       " 'you do not take this seriously',\n",
       " 'you get mad at me',\n",
       " 'you get paid',\n",
       " 'you keep forgetting',\n",
       " 'you keep saying',\n",
       " 'you look more like',\n",
       " 'you made me mad',\n",
       " 'you make me angry',\n",
       " 'you make me feel like i am',\n",
       " 'you make me mad',\n",
       " 'you mumble',\n",
       " 'you need a psychiatrist',\n",
       " 'you need to work harder',\n",
       " 'you pick up',\n",
       " 'you psycho',\n",
       " 'you should feel guilty',\n",
       " 'you should get more',\n",
       " 'you should loosen up',\n",
       " 'you should take more',\n",
       " 'you sound like hal',\n",
       " 'you sound like yoda',\n",
       " 'you try to hide it',\n",
       " 'yyou are uncultured'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
