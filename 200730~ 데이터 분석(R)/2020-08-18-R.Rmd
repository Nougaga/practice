---
title: Decision Tree, Ensemble
date: 2020-08-18
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(prul = TRUE)
options(max.print=10)
```

## **의사결정나무(Decision Tree)**

### 1. rpart
```{r prul=F}
library(rpart)
library(rpart.plot)
library(caret)  # confusionMatrix
```
```{r}
# 데이터 준비
# head(iris,51)
# data(iris)
idx <- createDataPartition(iris$Species, p=0.7, list=F) # 비율 고려 샘플링
train <- iris[idx,]
test <- iris[-idx,]

# 모델 적합(fitting)
m.rpart <- rpart(Species~., data=train) # rpart는 지니 계수를 기준으로
m.rpart
summary(m.rpart)

# plot
plot(m.rpart, margin=.2, compress=T)
text(m.rpart, cex=1.0)
# rpart.plot 패키지를 이용한 트리 그림
rpart.plot(m.rpart, type=4, extra=2, digits=3)
prp(m.rpart, type=4, extra=2, digits=3)

# 예측
m.rpart.pr <- predict(m.rpart, newdata=test, type='class')
m.rpart.pr

# 정확도 평가
table(m.rpart.pr, test$Species)
confusionMatrix(m.rpart.pr, test$Species)

# CP(Complexity Parameter) 확인
# plotcp(m.rpart)
m.rpart$cptable
m.rpart$cptable[2]

# prunning(사후 가지치기)
m.rpart.prune <- prune(m.rpart, cp=m.rpart$cptable[2])  # 2층까지만

# visualization
par(mfrow=c(1,2))
rpart.plot(m.rpart.prune, type=4, extra=2, digits=3)
rpart.plot(m.rpart, type=4, extra=2, digits=3)
```

### 2. party
```{r prul=F}
require(party)
```
```{r}
# ctree는 p값을 이용한 유의성 검정을 기준으로
m.ctree <- ctree(Species~., data=train)
m.ctree

# plot
plot(m.ctree)

# 예측
m.ctree.pr <- predict(m.ctree, newdata=test)
summary(m.ctree.pr)

# 정확도 평가
confusionMatrix(m.ctree.pr, test$Species)

# 과적합 방지
con <- ctree_control(maxdepth = 2)  # 사전 가지치기 (2층까지만)
m.ctree.prn <- ctree(Species~., data=train, controls=con)
plot(m.ctree.prn)



```
### 3. tree
```{r prul=F}
require(tree)
```
```{r}
# tree는 엔트로피 지수를 이용
m.tree <- tree(Species~., data=train)
m.tree

# plot
plot(m.tree)
text(m.tree)

# 예측
m.tree.pr <- predict(m.tree, newdata=test, type='class')
m.tree.pr

# 정확도 평가
confusionMatrix(m.tree.pr, test$Species)
```

---

## **앙상블**

### 1. bagging

bootstrap + aggregating

원데이터로부터 크기가 같은 표본을 여러 번 단순임의복원추출하여

각 표본에 대한 분류기를 생성한 후 앙상블

```{r prul=F}
library(adabag)
```
```{r}
# data load
data(iris)

# bagging
iris.bagging <- adabag::bagging(Species~., data=iris, mfinal=100)
# iris.bagging

iris.bagging$importance

# visualization
par(mfrow=c(1,2))
plot(iris.bagging$trees[[1]])
text(iris.bagging$trees[[1]])
plot(iris.bagging$trees[[11]])
text(iris.bagging$trees[[11]])

# prediction
pred1 <- predict(iris.bagging, newdata=iris[,-5])

# model accuracy
confusionMatrix(as.factor(pred1$class), as.factor(iris[,5]))
```
```{r prul=F}
# looking into model
summary(iris.bagging)
str(iris.bagging)
iris.bagging$votes
iris.bagging$samples[,1]

# checking sample data
sort(iris.bagging$samples[,1])
bg_sample1 <- table(sort(iris.bagging$samples[,1]))
bg_sample1
attributes(bg_sample1)$dimnames[[1]]
bg_sample11 <- data.frame(idx=as.numeric(attributes(bg_sample1)$dimnames[[1]]),
                          freq=bg_sample1)
bg_sample11
str(bg_sample11)
```

### 2. Boosting

배깅의 과정과 유사하나 재표본 과정에서 각 자료에 동일한 확률을 부여하는 것이 아니라,

분류가 잘못된 데이터에 더 큰 가중을 주어 표본을 추출

붓스트랩 표본을 추출하여 calssifier를 생성한 후

분류 결과를 이용하여 확률을 조정 --> 다음 표본을 추출

```{r prul=F}
library(adabag)
library(caret)
```

```{r}
# splitting data
idx <- createDataPartition(iris$Species, p=0.7, list=F)
iris.train <- iris[idx,]
iris.test <- iris[-idx,]

# boosting
iris.boosting <- adabag::boosting(Species~., data=iris.train, boos=T, mfinal=11)

# feature importance
iris.boosting$importance

# visualization
par(mfrow=c(1,2))
plot(iris.boosting$trees[[1]], margin=.2)
text(iris.boosting$trees[[1]])
plot(iris.boosting$trees[[11]], margin=.2)
text(iris.boosting$trees[[11]])

# prediction
pred_boosting <- predict(iris.boosting, newdata=iris.test[,-5])

# model accuracy
table(iris.boosting$class == iris.train$Species)

# test accuracy
caret::confusionMatrix(as.factor(pred_boosting$class), iris.test[,5])  # Accuracy : 0.9111

# tuning
BRANCH = 6
iris.boosting2 <- adabag::boosting(Species~., data=iris.train, boos=T, mfinal=BRANCH)
plot(iris.boosting2$trees[[BRANCH]], margin=.2); text(iris.boosting2$trees[[BRANCH]])
pred_boosting2 <- predict(iris.boosting2, newdata=iris.test[,-5])
table(iris.boosting2$class == iris.train$Species)
confusionMatrix(as.factor(pred_boosting2$class), iris.test[,5]) # Accuracy : 0.9111 별 차이 없음
```

### 3. Random Forest

배깅에 랜덤을 추가, 붓스트랩 샘플에 대해 모든 칼럼을 대상으로 분기를 검토하지 않고

사용할 feature를 임의로 추출하여 추출된 feature 중에 최적의 분할을 만들어낸다.

```{r prul=F}
library(randomForest)
```
```{r}
# splitting data
idx <- createDataPartition(iris$Species, p=0.7, list=F)
iris.train <- iris[idx,]
iris.test <- iris[-idx,]

# boosting 1
set.seed(1984)
iris.rf <- randomForest(Species~., data=iris.train,
                        ntree=100, mtry=sqrt(4), importance=T)
iris.rf

# prediction
pred_rf <- predict(iris.rf, newdata=iris.test[,-5])
pred_rf

# model accuracy
confusionMatrix(as.factor(pred_rf), as.factor(iris.test[,5]))

# importance
importance(iris.rf)

# importance(분산값 이용)
varImpPlot(iris.rf, main="varImpPlot of iris")  # 변수의 중요도를 파악하는 데에 참고할 그림
```