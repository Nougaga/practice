---
title: "200819-study"
author: "Nougaga"
date: '2020 8 19 '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## **군집 분석**

비지도(Unsupervised) 학습: label로 나누지 않음

- 군집 분석의 종류: 계층적 군집, 분할 군집, 자기 조직화 지도, ...

- 거리 계산: 유클리디안, 맨해튼, 민코우스키, 마할라노비스, ...

```{r results='hide', collapse=TRUE}
library(cluster)
library(caret)
```
```{r}
x <- matrix(1:9, nrow=3, by=T)

dist <- dist(x, method="euclidean")
dist

s <- sum((x[1,]-x[2,])^2)
sqrt(s)

dist(x, method="euclidean"); dist(x, method="minkowski", p=2)
dist(x, method="manhattan"); dist(x, method="minkowski", p=1)
```

### **1. 계층적 군집**

linkage methods

1) 최단연결법(single): 두 군집 사이의 관측값들 간의 거리 중 최솟값

2) 최장연결법(complete): 두 군집 사이의 관측값들 간의 거리 중 최댓값

3) 중심연결법(centroid): 두 군집의 중심 간의 거리

4) 평균연결법(average): 모든 값들 간의 거리 평균 --> 계산량이 많음

5) 와드 연결법(ward): 군집 간의 거리를 이용하는 다른 연결과는 달리, 군집 내의 오차제곱합(sum of square error)을 이용
  - 두 군집이 병합하면 이전의 오차제곱합보다 커지는데, 이 증가량이 작아지는 방향으로 군집을 형성

```{r}
hc <- hclust(dist)
hc
plot(hc)
```

```{r}
# 1. data load
data("USArrests")

# 2. 거리측정
d <- dist(USArrests, method = "euclidean")
# d

# 3. 계층적 군집 실시
# 3-1) ward 연결
clustering.w <- hclust(d, method = "ward.D")
clustering.w

# 3-2) single 연결
clustering.single <- hclust(d, method = "single")
clustering.single

# 3-3) average 연결
clustering.ave <- hclust(d, method = "average")
clustering.ave

# 4. 덴드로그램 확인
# ex) ward
plot(clustering.w)
rect.hclust(clustering.w, k = 2, border = 'red')  # 2 clusters가 생기도록 cutting
rect.hclust(clustering.w, h = 5, border = 5:6, which = c(1:8))  # height=5에서 cutting, 8 clusters만 보여줌

# 5. k(#.clusters) or h(height)로 군집 확인
grp1 <- cutree(clustering.ave, k=6)
grp1; str(grp1)
cluster.df <- data.frame(states = attributes(grp1)$names, group = grp1)
head(cluster.df)

grp2 <- cutree(clustering.ave, h=40)
cluster.df2 <- data.frame(states = attributes(grp2)$names, group = grp2)
head(cluster.df2)
```

### **2. 비계층적 군집**
#### k-means clustering

원하는 군집 수 만큼 초기값을 지정,

데이터를 가까운 초기값에 할당하여 군집을 형성한 뒤,

군집들의 평균을 재계산하여 초기값을 갱신

    + process
    1) k개의 군집을 임의 선택
    2) 각 데이터를 가까운 군집 중심에 할당
    3) 군집 내의 자료들의 평균을 계산하여 군집 중심을 갱신
    4) 군집 중심이 안정될 때까지 2)와 3)을 반복
    
    + 장점
    1) 알고리즘이 단순하며 빠름
    2) 계층적 군집보다 많은 데이터를 빠르게 처리
    
    + 단점
    1) 잡음이나 이상값에 영향을 많이 받음
    2) 비선형 데이터의 경우 성능이 떨어짐
       최적군집수 검토 필요
    
```{r results='hide', collapse=TRUE}
library(NbClust)
library(rattle)
library(dplyr)
```
```{r}
# data load
data(wine)
wines <- wine
wines %>% head
summary(wines)

# scaling(스케일 조정)
# normalizing(정규화): scale() --> 정규화 using standard deviation
wines.norm <- scale(wines[-1])
wines.norm %>% head
summary(wines.norm)

# checking
wines1 <- wine
norm_check.Alcohol <- sapply(wines1$Alcohol, function(x){(x-mean(wines1$Alcohol))/sqrt(var(wines1$Alcohol))})
table(norm_check.Alcohol == wines.norm[,1])


# cf) standardizing(표준화)
std_check.Alcohol <- sapply(wines1$Alcohol, function(x){(x-min(wines1$Alcohol))/(max(wines$Alcohol)-min(wines$Alcohol))})
plot(norm_check.Alcohol); points(std_check.Alcohol, col="blue")

# 스케일 조정
```
```{r}
std.preProc <- caret::preProcess(wines1, method=c("range"))
wines.std <- predict(std.preProc, wines1)
summary(wines.std)
# checking
table(std_check.Alcohol == wines.std$Alcohol)


# 스케일 조정2
norm.preProc <- caret::preProcess(wines1, method=c("center","scale"))
wines.norm2 <- predict(norm.preProc, wines1)
table(wines.norm2$Alcohol == wines.norm[,1])
```
```{r}
# compute the number of clusters
nc <- NbClust::NbClust(wines.norm, method="kmeans")  # the best number of clusters is  3 
table(nc$Best.nc[1,]) 
barplot(table(nc$Best.nc[1,]),
        xlab="num of clusters", ylab="num of criteria",
        ylim = c(0,20))
dim(wines.norm)
nc$Best.nc
# kmeans() modeling
cluster.kms <- kmeans(wines.norm, 3)
cluster.kms$size

# plotting
plot(wines.norm, col=cluster.kms$cluster)
points(cluster.kms$centers, col=1:3, pch=8, cex=1.5)

# metrics
confusionMatrix(as.factor(cluster.kms$cluster), wines$Type)
cluster.kms$cluster
wines$Type

cluster.kms$cluster2 <- ifelse(cluster.kms$cluster=="1","3",
                               ifelse(cluster.kms$cluster=="3","1","2"
                               ))
confusionMatrix(as.factor(cluster.kms$cluster2), wines$Type)  # Accuracy : 0.9663 
```